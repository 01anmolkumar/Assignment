{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01anmolkumar/Assignment/blob/main/Gen_AI_Intro_%26_Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH4uOtey8Do8"
      },
      "source": [
        "# Gen AI Intro & Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ_llcqh8N31"
      },
      "source": [
        "**Q1. What is Generative AI?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Generative AI is a branch of artificial intelligence that focuses on creating new and original data rather than only analyzing existing data.\n",
        "* It is capable of generating content such as text, images, audio, video, and code that resembles human-created data.\n",
        "* Generative AI models learn patterns, relationships, and structures from large datasets.\n",
        "\n",
        "* These models use probabilistic and deep learning techniques to produce realistic outputs.\n",
        "* Common generative models include n-gram models, Markov models, autoencoders, Variational Autoencoders (VAEs), GANs, and transformer-based models like GPT.\n",
        "* Generative AI is widely used in applications such as chatbots, content creation, image synthesis, recommendation systems, and creative design.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBLaSddl8Vb7"
      },
      "source": [
        "**Q2. How is Generative AI different from traditional AI?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Generative AI focuses on creating new data such as text, images, audio, or code, whereas traditional AI mainly focuses on analyzing data and making predictions or decisions.\n",
        "* Traditional AI systems are usually rule-based or discriminative, meaning they classify, detect, or predict outcomes based on given inputs.\n",
        "* Generative AI learns the underlying probability distribution of data and uses it to generate new, realistic outputs.\n",
        "\n",
        "* Traditional AI is commonly used for tasks like spam detection, image classification, and recommendation based on predefined objectives.\n",
        "* Generative AI is used for creative and content-generation tasks such as text generation, image synthesis, chatbots, and data augmentation.\n",
        "* In summary, traditional AI answers “what is this?”, while Generative AI answers “what can I create?”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eMUrqsD8WrE"
      },
      "source": [
        "**Q3. Name two applications of Generative AI in the industry.**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* One major application of Generative AI is **content creation**, where it is used to generate text, articles, marketing content, code, and chat responses.\n",
        "* Industries such as media, education, and software development widely use generative models for automation and creativity.\n",
        "\n",
        "* Another important application is **image and design generation**, where Generative AI creates images, logos, product designs, and visual content.\n",
        "* It is also used in industries like fashion, gaming, healthcare, and advertising to design prototypes and generate synthetic data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkbK26Lu8jyE"
      },
      "source": [
        "**Q4. What are some challenges associated with Generative AI?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* One major challenge of Generative AI is **data dependency**, as these models require very large and high-quality datasets for effective training.\n",
        "* Training generative models is **computationally expensive** and requires powerful hardware such as GPUs or TPUs.\n",
        "\n",
        "* Generative AI may produce **biased or incorrect outputs** if the training data contains bias or errors.\n",
        "* There are also challenges related to **hallucinations**, where models generate information that appears correct but is actually false.\n",
        "\n",
        "* Ethical concerns such as **misuse, deepfakes, copyright issues, and misinformation** are significant challenges.\n",
        "* Ensuring **control, reliability, and safety** of generated content remains an ongoing research problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkklq9j8rXN"
      },
      "source": [
        "**Q5. Why is Generative AI important for modern applications?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Generative AI is important for modern applications because it enables machines to create human-like content such as text, images, audio, and code.\n",
        "* It improves automation by reducing the need for manual content creation and repetitive tasks.\n",
        "\n",
        "* Generative AI enhances user experience in applications like chatbots, virtual assistants, and recommendation systems.\n",
        "* It supports innovation and creativity in fields such as media, design, healthcare, and education.\n",
        "\n",
        "* By learning complex patterns from large datasets, Generative AI helps build intelligent systems that adapt to user needs.\n",
        "* It also enables scalable, personalized, and efficient solutions across many industries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udXZL_8h8ycD"
      },
      "source": [
        "**Q6. What is probabilistic modeling in the context of Generative AI?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Probabilistic modeling in Generative AI refers to modeling the uncertainty and randomness present in data using probability distributions.\n",
        "* These models learn the probability distribution of data so they can generate new samples that are similar to the training data.\n",
        "\n",
        "* Instead of producing a single fixed output, probabilistic models estimate how likely different outputs are.\n",
        "* This allows Generative AI systems to produce diverse and realistic results.\n",
        "\n",
        "* Examples of probabilistic models include n-gram models, Markov models, Bayesian models, and Variational Autoencoders (VAEs).\n",
        "* Probabilistic modeling is essential for generating flexible, creative, and uncertainty-aware outputs in Generative AI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Bsh0yp84PX"
      },
      "source": [
        "**Q7. Define a generative model.**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* A generative model is a type of machine learning model that learns the underlying probability distribution of input data.\n",
        "* Its main goal is to generate new data samples that resemble the original training data.\n",
        "\n",
        "* Generative models can create text, images, audio, or other forms of data.\n",
        "* They model how data is generated rather than just distinguishing between classes.\n",
        "\n",
        "* Common examples of generative models include n-gram models, Markov models, GANs, VAEs, and transformer-based models.\n",
        "* Generative models are widely used in content generation, data augmentation, and simulation tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7aZGPR59Asu"
      },
      "source": [
        "**Q8. Explain how an n-gram model works in text generation.**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* An n-gram model is a probabilistic language model that predicts the next word based on the previous (n−1) words.\n",
        "* It works by analyzing a large text corpus and calculating the probability of word sequences of length n.\n",
        "\n",
        "* During text generation, the model selects the next word based on the highest probability given the previous words.\n",
        "* For example, a bigram model (n=2) predicts a word using only the immediately preceding word.\n",
        "\n",
        "* N-gram models are simple and easy to implement but rely heavily on observed word sequences.\n",
        "* They are commonly used as baseline models in text generation and language modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j0737Gy9HpS"
      },
      "source": [
        "**Q9. What are the limitations of n-gram models?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* N-gram models suffer from data sparsity, as many possible word combinations may not appear in the training data.\n",
        "* They cannot effectively handle long-range dependencies because they only consider a fixed number of previous words.\n",
        "\n",
        "* N-gram models require large amounts of text data to estimate probabilities accurately.\n",
        "* They perform poorly when dealing with rare or unseen words.\n",
        "\n",
        "* These models have limited understanding of grammar and semantics.\n",
        "* As a result, the generated text may lack coherence and meaningful structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YfIm_hb9OJo"
      },
      "source": [
        "**Q10. How can you improve the performance of an n-gram model?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* The performance of an n-gram model can be improved by increasing the value of n to capture more contextual information.\n",
        "* Smoothing techniques such as Laplace smoothing, Good-Turing smoothing, or Kneser-Ney smoothing can be applied to handle zero probabilities.\n",
        "\n",
        "* Using a larger and more diverse training corpus helps reduce data sparsity.\n",
        "* Pruning rare n-grams can improve efficiency and reduce noise.\n",
        "\n",
        "* Backoff and interpolation methods can be used to combine lower-order and higher-order n-gram models.\n",
        "* These techniques help improve prediction accuracy and text generation quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhVP7fO59WMI"
      },
      "source": [
        "**Q11. What is the Markov assumption, and how does it apply to text generation?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* The Markov assumption states that the probability of a future event depends only on a limited number of previous events, not the entire history.\n",
        "* In text generation, this means the next word depends only on the previous (n−1) words.\n",
        "\n",
        "* This assumption simplifies language modeling by reducing computational complexity.\n",
        "* It is the foundation of n-gram and Markov models used in text generation.\n",
        "\n",
        "* While the Markov assumption makes models efficient and easy to implement, it limits their ability to capture long-range dependencies in text.\n",
        "* As a result, generated text may lack global coherence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47yZRkvX9cVm"
      },
      "source": [
        "**Q12. Why are probabilistic models important in Generative AI?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Probabilistic models are important in Generative AI because they model uncertainty and variability present in real-world data.\n",
        "* They learn the probability distribution of data, which allows them to generate multiple possible outputs instead of a single fixed result.\n",
        "\n",
        "* These models help produce diverse and realistic content such as text, images, and audio.\n",
        "* Probabilistic modeling enables better handling of ambiguous and incomplete data.\n",
        "\n",
        "* By estimating likelihoods, probabilistic models support creativity, flexibility, and robustness in generative systems.\n",
        "* They form the foundation of many generative approaches such as n-gram models, Bayesian models, and VAEs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqJqNtC9iof"
      },
      "source": [
        "**Q13. What is an autoencoder?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* An autoencoder is a type of neural network used to learn an efficient representation of input data.\n",
        "* It consists of two main parts: an encoder and a decoder.\n",
        "\n",
        "* The encoder compresses the input data into a lower-dimensional representation called the latent space.\n",
        "* The decoder reconstructs the original data from this latent representation.\n",
        "\n",
        "* Autoencoders are mainly used for dimensionality reduction, feature learning, and data compression.\n",
        "* They are also used in tasks such as noise removal and anomaly detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qvjb8Bu9plL"
      },
      "source": [
        "**Q14. How does a VAE differ from a standard autoencoder?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* A Variational Autoencoder (VAE) differs from a standard autoencoder in how it represents the latent space.\n",
        "* A standard autoencoder learns a fixed latent representation, while a VAE learns a **probabilistic latent distribution**.\n",
        "\n",
        "* In a VAE, the encoder outputs parameters (mean and variance) of a probability distribution instead of a single point.\n",
        "* This allows VAEs to generate new data by sampling from the latent space.\n",
        "\n",
        "* VAEs include an additional **KL divergence loss** to regularize the latent space.\n",
        "* As a result, VAEs are more suitable for **generative tasks**, whereas standard autoencoders are mainly used for reconstruction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88GVcSGm9wiy"
      },
      "source": [
        "**Q15. Why are VAEs useful in generative modeling?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Variational Autoencoders (VAEs) are useful in generative modeling because they learn a structured and continuous latent space.\n",
        "* This allows the model to generate new and meaningful data samples by sampling from the latent space.\n",
        "\n",
        "* VAEs combine probabilistic modeling with deep learning, which helps capture uncertainty in data.\n",
        "* They produce smooth and diverse outputs compared to traditional autoencoders.\n",
        "\n",
        "* VAEs are widely used for text, image, and data generation tasks.\n",
        "* Their regularized latent space makes them stable and effective for generative applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orY62mpDE6Gn"
      },
      "source": [
        "**Q16. What role does the decoder play in an autoencoder**\n",
        "\n",
        "* The decoder is responsible for reconstructing the original input from the latent representation produced by the encoder.\n",
        "* It transforms the compressed, lower-dimensional latent vector back into the original data space, aiming to generate output as close as possible to the input.\n",
        "* In standard autoencoders, the decoder learns a deterministic mapping to minimize reconstruction loss.\n",
        "* In Variational Autoencoders (VAEs), the decoder works probabilistically: it generates outputs from latent vectors sampled from the learned distribution (mean and variance), allowing the creation of new, realistic data.\n",
        "* The decoder is critical in generative tasks because the quality of the generated samples depends on how well it interprets the latent space.\n",
        "* Essentially, the encoder compresses information, while the decoder translates this compressed representation back into meaningful data, completing the end-to-end learning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHsP63ahFIk1"
      },
      "source": [
        "**Q17. How does the latent space affect text generation in a VAE**\n",
        "\n",
        "* The latent space in a Variational Autoencoder (VAE) represents a compressed, abstract representation of input data in a continuous, structured form.\n",
        "* For text generation, this latent space captures semantic and syntactic patterns from the training text, allowing the model to generate coherent and meaningful sentences.\n",
        "* By sampling different points from the latent space, the VAE can produce diverse text outputs that maintain the characteristics of the original data.\n",
        "* The structure and smoothness of the latent space, enforced by the KL divergence term, ensure that small changes in the latent vector lead to gradual and logical changes in generated text.\n",
        "* This makes the latent space crucial for controlling the style, content, or attributes of generated text, enabling tasks like conditional text generation or interpolation between different sentences.\n",
        "* Overall, the latent space serves as the “creative” engine of the VAE, guiding how encoded knowledge is decoded into new, plausible text sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gl_L2kuGM91"
      },
      "source": [
        "**Q18. What is the purpose of the Kullback-Leibler (KL) divergence term in VAEs**\n",
        "\n",
        "* KL divergence acts as a regularizer for the latent space.  \n",
        "* It measures the difference between the learned latent distribution and the prior distribution (usually standard normal).  \n",
        "* Ensures the latent space is smooth and continuous.  \n",
        "* Prevents the latent vectors from collapsing into arbitrary regions.  \n",
        "* Enables meaningful interpolation between points in the latent space.  \n",
        "* Supports sampling for generating new, realistic data.  \n",
        "* Balances reconstruction quality and generative capability.  \n",
        "* Improves generalization to unseen data by preventing overfitting.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6wpABs6GUKj"
      },
      "source": [
        "**Q19. How can you prevent overfitting in a VAE**\n",
        "\n",
        "* Apply regularization techniques like dropout or weight decay in the encoder and decoder.  \n",
        "* Use early stopping based on validation loss to avoid over-training.  \n",
        "* Increase the diversity and size of the training dataset.  \n",
        "* Adjust the weight of the KL divergence term to maintain a smooth latent space.  \n",
        "* Limit the latent dimension size to reduce model capacity.  \n",
        "* Simplify the network architecture if it is too complex.  \n",
        "* Use data augmentation to expose the model to varied inputs.  \n",
        "* Monitor reconstruction and KL losses to ensure balanced learning.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnJdDeE_GY_4"
      },
      "source": [
        "**Q20. Explain why VAEs are commonly used for unsupervised learning tasks**\n",
        "\n",
        "* VAEs can learn meaningful latent representations without labeled data.  \n",
        "* They capture the underlying data distribution in a structured latent space.  \n",
        "* The probabilistic encoding allows generation of new, realistic samples.  \n",
        "* KL divergence ensures smoothness and regularity in the latent space.  \n",
        "* VAEs are effective for dimensionality reduction and feature learning.  \n",
        "* They enable interpolation and manipulation of data in the latent space.  \n",
        "* Suitable for tasks like anomaly detection, image synthesis, and text generation.  \n",
        "* They generalize well to unseen data due to probabilistic constraints.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7EGDrn0GcWP"
      },
      "source": [
        "**Q21. What is a transformer model**\n",
        "\n",
        "* A transformer is a deep learning model designed for handling sequential data like text.  \n",
        "* It relies on self-attention mechanisms instead of recurrent layers.  \n",
        "* Processes all input tokens in parallel, improving training efficiency.  \n",
        "* Captures long-range dependencies between tokens effectively.  \n",
        "* Consists of encoder and decoder stacks in the original architecture.  \n",
        "* Widely used in NLP tasks such as translation, summarization, and text generation.  \n",
        "* Supports scalability to very large datasets and models.  \n",
        "* Forms the foundation for models like BERT and GPT.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDN85iIuGffh"
      },
      "source": [
        "**Q22. Explain the purpose of self-attention in transformers**\n",
        "\n",
        "* Self-attention allows the model to weigh the importance of each token relative to others in the sequence.  \n",
        "* It captures relationships between distant words, enabling understanding of context.  \n",
        "* Helps the model focus on relevant parts of the input for each output token.  \n",
        "* Enables parallel processing of sequences, unlike RNNs.  \n",
        "* Supports learning of complex dependencies in language data.  \n",
        "* Improves performance on tasks like translation, summarization, and question answering.  \n",
        "* Forms the core mechanism that allows transformers to scale effectively.  \n",
        "* Reduces the need for sequential computation, speeding up training and inference.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3sAL0bOGhof"
      },
      "source": [
        "**Q23. How does a GPT model generate text**\n",
        "\n",
        "* GPT generates text using a transformer decoder architecture.  \n",
        "* It predicts the next token in a sequence based on previously generated tokens.  \n",
        "* Uses self-attention to capture context from all previous tokens efficiently.  \n",
        "* Outputs probabilities for each token and selects the most likely one (or samples probabilistically).  \n",
        "* Generates text iteratively, feeding each predicted token back into the model.  \n",
        "* Can produce coherent and contextually relevant sentences over long sequences.  \n",
        "* The quality of output improves with larger models and extensive pre-training on diverse datasets.  \n",
        "* Supports tasks like story writing, summarization, and dialogue generation without task-specific training.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY6PVy9mGm_9"
      },
      "source": [
        "**Q24. What are the key differences between a GPT model and an RNN**\n",
        "\n",
        "* GPT uses a transformer architecture, while RNNs rely on recurrent connections.  \n",
        "* GPT processes sequences in parallel, whereas RNNs process tokens sequentially.  \n",
        "* Transformers like GPT handle long-range dependencies better than RNNs.  \n",
        "* GPT employs self-attention to focus on relevant tokens, while RNNs rely on hidden states.  \n",
        "* GPT scales efficiently to very large models, unlike RNNs which suffer from vanishing gradients.  \n",
        "* Pre-trained GPT models can perform zero-shot or few-shot tasks, which RNNs struggle with.  \n",
        "* GPT supports large context windows, enabling coherent long-text generation.  \n",
        "* RNNs are simpler and lighter but less effective for modern large-scale NLP tasks.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WkFvIpYGqxa"
      },
      "source": [
        "**Q25. How does fine-tuning improve a pre-trained GPT model**\n",
        "\n",
        "* Fine-tuning adapts a pre-trained GPT model to a specific task or domain.  \n",
        "* It updates model weights using task-specific labeled data while retaining general knowledge.  \n",
        "* Helps improve accuracy and relevance for specialized applications.  \n",
        "* Reduces the amount of data and training time needed compared to training from scratch.  \n",
        "* Allows the model to learn domain-specific vocabulary and context.  \n",
        "* Can improve performance on tasks like summarization, classification, or question answering.  \n",
        "* Maintains the generative capabilities of the pre-trained model while optimizing for target tasks.  \n",
        "* Fine-tuning balances general language understanding with task-specific expertise.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnigerDnGv1E"
      },
      "source": [
        "**Q26. What is zero-shot learning in the context of GPT models**\n",
        "\n",
        "* Zero-shot learning allows GPT models to perform tasks without explicit task-specific training.  \n",
        "* The model uses its pre-trained knowledge to understand and respond to new tasks.  \n",
        "* Tasks are communicated through natural language prompts rather than labeled examples.  \n",
        "* Enables flexibility to handle multiple tasks with a single model.  \n",
        "* Reduces the need for large annotated datasets for every new task.  \n",
        "* Performance depends on the quality and clarity of the prompt.  \n",
        "* Useful for classification, summarization, translation, and reasoning tasks.  \n",
        "* Demonstrates the generalization ability of large pre-trained language models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOjAofw_Gzk2"
      },
      "source": [
        "**Q27. Describe how prompt engineering can impact GPT model performance**\n",
        "\n",
        "* Prompt engineering involves carefully designing input prompts to guide the model’s responses.  \n",
        "* The phrasing, context, and structure of the prompt can significantly affect output quality.  \n",
        "* Well-crafted prompts can improve relevance, coherence, and accuracy of generated text.  \n",
        "* Poorly designed prompts may produce ambiguous, irrelevant, or incorrect responses.  \n",
        "* Techniques like few-shot examples or explicit instructions can enhance model understanding.  \n",
        "* Prompt engineering allows control over tone, style, and format of outputs.  \n",
        "* It is a critical tool for zero-shot and few-shot learning scenarios.  \n",
        "* Effective prompt engineering reduces the need for extensive fine-tuning.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmxO-r7G2t2"
      },
      "source": [
        "**Q28. Why are large datasets essential for training GPT models**\n",
        "\n",
        "* Large datasets provide diverse language examples, improving generalization.  \n",
        "* They expose the model to various topics, styles, and contexts.  \n",
        "* More data reduces overfitting and improves robustness on unseen inputs.  \n",
        "* Large-scale data helps capture rare words and complex patterns in language.  \n",
        "* It enhances the model’s ability to perform zero-shot and few-shot tasks.  \n",
        "* Extensive datasets improve fluency, coherence, and factual accuracy in outputs.  \n",
        "* The quality and size of data directly impact the model’s overall performance.  \n",
        "* Large datasets are crucial for training very large models with billions of parameters.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoCYha1WHAVe"
      },
      "source": [
        "**Q29. What are potential ethical concerns with GPT models**\n",
        "\n",
        "* GPT models can generate biased or offensive content learned from training data.  \n",
        "* They may produce misinformation or hallucinated facts.  \n",
        "* Models can be misused for spam, deepfakes, or malicious automation.  \n",
        "* Data privacy issues arise if sensitive information is included in training data.  \n",
        "* Overreliance on AI outputs may reduce human critical thinking.  \n",
        "* Large models consume significant computational resources, raising environmental concerns.  \n",
        "* Lack of transparency in decision-making can hinder accountability.  \n",
        "* Ethical deployment requires careful monitoring, bias mitigation, and responsible usage policies.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1rLIhSHDbl"
      },
      "source": [
        "**Q30. How does the attention mechanism contribute to GPT’s ability to handle long-range dependencies**\n",
        "\n",
        "* Attention allows GPT to consider the relevance of all previous tokens when generating the next token.  \n",
        "* It captures dependencies between distant words that RNNs struggle with.  \n",
        "* Self-attention computes weighted relationships, giving more focus to important context.  \n",
        "* Enables the model to maintain coherence across long sequences.  \n",
        "* Supports parallel processing of tokens, improving efficiency.  \n",
        "* Helps in understanding complex sentence structures and multi-sentence context.  \n",
        "* Facilitates better reasoning and context retention over extended text.  \n",
        "* Is fundamental to the transformer architecture that underpins GPT models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NuGYqdAHFq0"
      },
      "source": [
        "**Q31. What are some limitations of GPT models for real-world applications**\n",
        "\n",
        "* GPT models can generate incorrect or misleading information.  \n",
        "* They may produce biased or harmful content learned from training data.  \n",
        "* Require large computational resources for training and deployment.  \n",
        "* Struggle with reasoning tasks requiring deep understanding or world knowledge.  \n",
        "* Limited ability to verify facts or update knowledge in real time.  \n",
        "* May overfit to frequent patterns, ignoring rare but important context.  \n",
        "* Performance heavily depends on prompt quality and design.  \n",
        "* Deployment in sensitive areas requires careful monitoring and safeguards.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GeGfS-cHJ4S"
      },
      "source": [
        "**Q32. How can GPT models be adapted for domain-specific text generation**\n",
        "\n",
        "* Fine-tuning on domain-specific data helps the model learn relevant terminology and style.  \n",
        "* Using carefully designed prompts guides the model to generate context-appropriate content.  \n",
        "* Incorporating retrieval-augmented generation allows access to up-to-date or specialized knowledge.  \n",
        "* Adjusting temperature and sampling parameters controls creativity and adherence to domain rules.  \n",
        "* Domain-specific evaluation metrics ensure the generated text meets quality standards.  \n",
        "* Combining GPT with smaller expert models can improve accuracy and reliability.  \n",
        "* Active monitoring and human-in-the-loop review mitigate errors in critical domains.  \n",
        "* Continuous domain adaptation helps maintain relevance as new information emerges.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9DWiMZHOen"
      },
      "source": [
        "**Q33. What are some common metrics for evaluating text generation quality**\n",
        "\n",
        "* BLEU (Bilingual Evaluation Understudy) measures n-gram overlap with reference text.  \n",
        "* ROUGE (Recall-Oriented Understudy for Gisting Evaluation) evaluates recall of overlapping n-grams, especially for summarization.  \n",
        "* METEOR considers precision, recall, and synonym matches for text similarity.  \n",
        "* Perplexity measures how well a model predicts a sequence of words.  \n",
        "* Cosine similarity between embeddings evaluates semantic similarity.  \n",
        "* Human evaluation assesses fluency, coherence, relevance, and creativity.  \n",
        "* Diversity metrics (e.g., distinct n-grams) measure variation in generated text.  \n",
        "* Task-specific metrics, such as factual accuracy or sentiment alignment, ensure outputs meet practical requirements.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7YQi_sVHV-y"
      },
      "source": [
        "**Q34. Explain the difference between deterministic and probabilistic text generation**\n",
        "\n",
        "* Deterministic text generation produces the same output for a given input every time, often using greedy decoding.  \n",
        "* Probabilistic text generation samples tokens based on predicted probabilities, introducing randomness.  \n",
        "* Deterministic methods ensure consistency but may lack diversity and creativity.  \n",
        "* Probabilistic methods allow varied and more human-like outputs across generations.  \n",
        "* Deterministic approaches are simpler and faster but may get stuck in repetitive patterns.  \n",
        "* Probabilistic approaches can explore less likely but contextually relevant words.  \n",
        "* Temperature and top-k/top-p sampling control randomness in probabilistic generation.  \n",
        "* Choosing between the two depends on whether consistency or creativity is prioritized.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO0AU1XmHa06"
      },
      "source": [
        "**Q35. How does beam search improve text generation in language models**\n",
        "\n",
        "* Beam search explores multiple possible sequences simultaneously instead of selecting only the most probable token at each step.  \n",
        "* It maintains a fixed number of top sequences (beams) to consider at each step, balancing exploration and exploitation.  \n",
        "* Helps avoid locally optimal but globally suboptimal sequences.  \n",
        "* Produces more coherent and fluent outputs compared to greedy decoding.  \n",
        "* Can be combined with length normalization to prevent bias toward shorter sequences.  \n",
        "* Improves performance in tasks like translation, summarization, and dialogue generation.  \n",
        "* Increases the likelihood of generating high-quality and contextually accurate text.  \n",
        "* Adjustable beam width allows tuning between diversity and computation cost.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVJkVSJnHgtM"
      },
      "source": [
        "# practical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jtvi9aCH42r"
      },
      "source": [
        "**Q1.Write a code to generate a random sentence using probabilistic modeling\n",
        "(Markov Chain). Use the sentence \"The cat is on the mat\" as an exampl@**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M9UIJ4q8GRZ",
        "outputId": "b97f0b85-5c38-48a3-fa3a-4aef766d0227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat is on the mat\n"
          ]
        }
      ],
      "source": [
        " import random\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cat is on the mat\"\n",
        "words = sentence.split()\n",
        "\n",
        "# Build Markov Chain model (word -> next word probabilities)\n",
        "markov_chain = {}\n",
        "for i in range(len(words) - 1):\n",
        "    if words[i] not in markov_chain:\n",
        "        markov_chain[words[i]] = []\n",
        "    markov_chain[words[i]].append(words[i+1])\n",
        "\n",
        "# Function to generate random sentence\n",
        "def generate_sentence(chain, start_word, length=5):\n",
        "    word = start_word\n",
        "    result = [word]\n",
        "    for _ in range(length-1):\n",
        "        next_words = chain.get(word, None)\n",
        "        if not next_words:\n",
        "            break\n",
        "        word = random.choice(next_words)\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Generate a random sentence starting with \"The\"\n",
        "random_sentence = generate_sentence(markov_chain, start_word=\"The\", length=6)\n",
        "print(random_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM_koeuvIj9H"
      },
      "source": [
        "**Q2.Build a simple Autoencoder model using Keras to learn a compressed\n",
        "representation of a given sentence. Use a dataset of your choice**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsq_TljmIU5P",
        "outputId": "4dfb9f29-b2de-4bf3-da45-958cbab3e006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n",
            "Compressed representation: [[0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Activation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample dataset\n",
        "sentences = [\n",
        "    \"The cat is on the mat\",\n",
        "    \"Dogs are playing in the yard\",\n",
        "    \"Birds are flying in the sky\",\n",
        "    \"The sun rises in the east\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "data = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Autoencoder architecture\n",
        "input_dim = max_len\n",
        "encoding_dim = 3  # compressed representation size\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "\n",
        "# Decoder: map from encoding_dim back to max_len * vocab_size\n",
        "# Then reshape to (max_len, vocab_size) and apply softmax across vocab_size\n",
        "decoded_flat = Dense(input_dim * vocab_size, activation='linear')(encoded) # Use linear activation before final softmax\n",
        "decoded_reshaped = Reshape((input_dim, vocab_size))(decoded_flat)\n",
        "decoded = Activation('softmax')(decoded_reshaped) # Softmax applied on the last axis (vocab_size)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Prepare data for training (y_train should be just data, not expanded)\n",
        "x_train = data\n",
        "y_train = data # Corrected y_train\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(x_train, y_train, epochs=100, batch_size=2, verbose=0)\n",
        "\n",
        "# Encoder model to get compressed representation\n",
        "encoder = Model(input_layer, encoded)\n",
        "\n",
        "# Example: get compressed representation of first sentence\n",
        "compressed = encoder.predict(x_train[0:1])\n",
        "print(\"Compressed representation:\", compressed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvrPJRiSKBX5"
      },
      "source": [
        "Q3.Use the Hugging Face transformers library to fine-tune a pre-trained GPT-2\n",
        "model on a custom text data and generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "awYEadAfJVwQ",
        "outputId": "9163b3af-2fb5-4855-8a8a-eb6ca583c435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " Once upon a time, Where the Father was, had I found you?\n",
            "\n",
            "Who were the children of the dead?\n",
            "\n",
            "Who is the child?\n",
            "\n",
            "The Father hath seen the Son;\n",
            "\n",
            "\n",
            "the Son of the woman of\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "# 1. Prepare custom text data\n",
        "text_data = \"\"\"Once upon a time, there was a brave knight.\n",
        "The sun rises in the east and sets in the west.\n",
        "Cats are playful and curious animals.\n",
        "Dogs love running in the yard.\n",
        "Birds are singing beautifully in the morning.\"\"\"\n",
        "with open(\"custom_text.txt\", \"w\") as f:\n",
        "    f.write(text_data)\n",
        "\n",
        "# 2. Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 3. Add padding token if not exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 4. Tokenize data\n",
        "with open(\"custom_text.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "encodings = tokenizer(lines, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
        "\n",
        "# 5. Create Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_ids': encodings['input_ids'],\n",
        "    'attention_mask': encodings['attention_mask']\n",
        "})\n",
        "\n",
        "# 6. Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 7. Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    prediction_loss_only=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 8. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# 9. Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# 10. Generate text using the fine-tuned model\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50, num_return_sequences=1, do_sample=True, temperature=0.8)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOVZO8BXMJt6"
      },
      "source": [
        "**Q4.Implement a text generation model using a simple Recurrent Neural\n",
        "Network (RNN) in Keras. Train the model on a custom data and generate a\n",
        "word**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wjnsnIjKUqh",
        "outputId": "7dcd0d01-a279-44af-a8d9-86731a5e4304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: The cat is on the mat the dog is in the yard\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Sample text data\n",
        "text = \"\"\"The cat is on the mat. The dog is in the yard. The bird is in the sky.\n",
        "Cats and dogs are friendly. Birds love to sing. The sun rises in the east.\"\"\"\n",
        "\n",
        "# 2. Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 3. Create input sequences\n",
        "input_sequences = []\n",
        "words = text.split()\n",
        "for i in range(1, len(words)):\n",
        "    seq = words[:i+1]\n",
        "    input_sequences.append(tokenizer.texts_to_sequences([' '.join(seq)])[0])\n",
        "\n",
        "# 4. Pad sequences\n",
        "max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# 5. Split into features and label\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# 6. Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50))  # embedding size = 50\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train the model\n",
        "model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# 8. Generate text\n",
        "seed_text = \"The cat\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "\n",
        "    # Get word from index\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdjh0cZATwtW"
      },
      "source": [
        "**Q5. Write a program to generate a sequence of text using an LSTM-based\n",
        "model in TensorFlow, trained on a custom data of sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ossAekhUMgH3",
        "outputId": "17ab6773-d489-44c7-ed9a-f19206cb0cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: The cat is on the mat mat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# 1. Custom dataset of sentences\n",
        "sentences = [\n",
        "    \"The cat is on the mat\",\n",
        "    \"The dog is in the yard\",\n",
        "    \"Birds are flying in the sky\",\n",
        "    \"The sun rises in the east\",\n",
        "    \"The sun sets in the west\"\n",
        "]\n",
        "\n",
        "# 2. Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 3. Create input sequences\n",
        "input_sequences = []\n",
        "for line in sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# 4. Pad sequences\n",
        "max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# 5. Split into features and labels\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# 6. Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50))  # embedding size 50\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train the model\n",
        "model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# 8. Generate text\n",
        "seed_text = \"The cat\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yKaO3dGVmsA"
      },
      "source": [
        " **Q6.Build a program that uses GPT-2 from Hugging Face to generate a story\n",
        "based on a custom prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-urZ0JUVG5Z",
        "outputId": "a91a957a-7fba-4807-eed1-1830308e4689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story:\n",
            " Once upon a time, in a small village, there lived a curious young girl named Anna, who had been a girl all her life, and was now being treated as a girl. She was the daughter of one of the most illustrious men in the town. She had been living in a house she had built on a hill that had been set on fire. Anna had been in a state of disarray and disordered, and had become a prostitute, a slave, and a woman. At one time her mother had been an abattoir keeper and had been in the business of stealing from the poor poor woman. In the middle of the night, as Anna was sleeping, she heard a voice call out from the woods, \"A man who is not living,\" and then the voice said, \"Come out, you have been robbed, and I will show you the way.\"\n",
            "\n",
            "The townspeople knew that the man was a thief, a fellow thief, and so they brought Anna\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# 1. Load pre-trained GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Handle missing padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 3. Define a custom prompt\n",
        "prompt = \"Once upon a time, in a small village, there lived a curious young girl named Anna\"\n",
        "\n",
        "# 4. Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 5. Generate story\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=200,        # maximum length of generated story\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,        # sampling for creative generation\n",
        "    top_k=50,              # consider top 50 tokens at each step\n",
        "    top_p=0.95,            # nucleus sampling\n",
        "    temperature=0.8        # creativity factor\n",
        ")\n",
        "\n",
        "# 6. Decode and print generated story\n",
        "story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Story:\\n\", story)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgiqF0GNWiKh"
      },
      "source": [
        "**Q7.Write a code to implement a simple text generation model using a GRUbased architecture in Keras**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlDpwSZ7Vxsc",
        "outputId": "e12cb699-d522-4e01-ac74-5d3e4ce6d05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: The cat is on the mat mat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Sample dataset\n",
        "sentences = [\n",
        "    \"The cat is on the mat\",\n",
        "    \"The dog is in the yard\",\n",
        "    \"Birds are flying in the sky\",\n",
        "    \"The sun rises in the east\",\n",
        "    \"The sun sets in the west\"\n",
        "]\n",
        "\n",
        "# 2. Tokenize sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 3. Create input sequences (n-grams)\n",
        "input_sequences = []\n",
        "for line in sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# 4. Pad sequences\n",
        "max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# 5. Split features and labels\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# 6. Build GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50))  # embedding size 50\n",
        "model.add(GRU(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train the model\n",
        "model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# 8. Generate text\n",
        "seed_text = \"The cat\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELoZ3-ScW13S"
      },
      "source": [
        "**Q8.Create a script to implement GPT-2-based text generation with beam\n",
        "search decoding to generate text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLe72XFqWgsY",
        "outputId": "a0fd9d16-9afa-4e57-de15-369df7448b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text with Beam Search:\n",
            " In a distant future, humans have colonized Mars and colonized the solar system, but they've never been able to colonize the solar system.\n",
            "\n",
            "In a distant future, humans have colonized Mars and colonized the solar system, but they've never been able to colonize the solar system.\n",
            "\n",
            "In a distant future, humans have colonized Mars and colonized the solar system, but they've never been able to colonize the solar system.\n",
            "\n",
            "In a distant future, humans have colonized Mars and colonized the solar system, but they've never been able to colonize the solar system.\n",
            "\n",
            "In a distant future, humans have colonized Mars and colonized the solar system, but they've never been\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# 1. Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Handle missing padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 3. Define a custom prompt\n",
        "prompt = \"In a distant future, humans have colonized Mars and\"\n",
        "\n",
        "# 4. Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 5. Generate text using beam search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=150,          # maximum number of tokens\n",
        "    num_beams=5,             # beam width\n",
        "    early_stopping=True,     # stop when all beams finish\n",
        "    num_return_sequences=1   # number of generated sequences\n",
        ")\n",
        "\n",
        "# 6. Decode and print the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text with Beam Search:\\n\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJGcQ1oJXmwV"
      },
      "source": [
        "**Q9.Implement a text generation script using GPT-2 with a custom temperature\n",
        "setting for diversity in output text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufhvJFb7XLCQ",
        "outputId": "de080eba-fce5-49be-bfae-afcd36c3eb30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text with Temperature = 1.2 :\n",
            " Once upon a time, a mysterious traveler arrived in the village and discovered that he'd been caught up in an illegal plot. With his new job, he was forced to confront the mysterious stranger in order to save a woman, the woman from who had become trapped after being rescued by a mysterious group of pirates. Along the way, he took out their treasure and was taken back.\n",
            "\n",
            "Contents show]\n",
            "\n",
            "In The Last Knight: Beyond Earth Volume 1: Legend of the Lost, the book is expanded to include the new story arc. In that story arc, it would not take the story arcs out into the world where the story was told.\n",
            "\n",
            "As the story was told by Captain Hook that took place on a voyage in the ocean\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# 1. Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Handle missing padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 3. Define a custom prompt\n",
        "prompt = \"Once upon a time, a mysterious traveler arrived in the village and\"\n",
        "\n",
        "# 4. Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 5. Generate text with custom temperature\n",
        "temperature_value = 1.2  # higher = more creative/diverse, lower = more deterministic\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=150,       # maximum number of tokens\n",
        "    do_sample=True,       # enable sampling\n",
        "    temperature=temperature_value,\n",
        "    top_k=50,             # consider top 50 tokens at each step\n",
        "    top_p=0.95,           # nucleus sampling\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "# 6. Decode and print generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text with Temperature =\", temperature_value, \":\\n\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK7X74BJYEyl"
      },
      "source": [
        "**Q10.Create a script to implement temperature sampling with GPT-2,\n",
        "experimenting with different values to generate creative text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCtm-UkwXvZ-",
        "outputId": "3439b904-3245-4e96-eb8a-5a21a110ccf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text with Temperature 0.7:\n",
            "In a futuristic city, robots and humans coexist peacefully, but the human race is still not quite ready to share its knowledge.\n",
            "\n",
            "Called \"The Robots of Mars,\" the Mars One team is trying to change that, by creating a robotic vehicle that will travel through the Martian sky. Called the Mars One Mission, the rover will be built on a giant robotic rocket, which will make a journey through the Martian sky, landing on the surface of Mars.\n",
            "\n",
            "\"We know that Mars is a very dangerous place to live and that there is a lot of danger here, so we've designed a vehicle that will be safe and secure,\" said Mars One principal investigator Michael C. Brown. \"This is a very complex project and we will\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text with Temperature 1.0:\n",
            "In a futuristic city, robots and humans coexist peacefully, but not necessarily in a peaceful way. Humans are not really intelligent, so they're not really aware of themselves and their interactions. Their minds are not even remotely good at predicting a particular situation or the future, so their behaviors are less developed and less likely to lead to any kind of progress. Robots aren't really human, they're just machines with different brains, but if it weren't for them they'd be better off just keeping going and doing stuff.\"\n",
            "\n",
            "Dr. Eun Hee of MIT's Computer Science Department says that his team used an algorithm to solve this problem. \"We looked at things like how long it takes each human to complete a task, how much work\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Text with Temperature 1.3:\n",
            "In a futuristic city, robots and humans coexist peacefully, but only against each other. In the center of our collective reality, life itself has nothing but fear; we are constantly in the grip of unimaginable dangers - the unknown, the unknown: life is all-embracing for humans. The city is only a stepping stone to this realization: in the very heart of their universe there are no humans, but, even as the world begins anew, there remains no sign of hope. As robots and humans alike journey through this universe, it is our responsibility to avoid any and all hazards before they are too big to pass by.\n",
            "\n",
            "The city of Aemar is a huge part of the galaxy where I once left home - one where\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# 1. Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Handle missing padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 3. Custom prompt\n",
        "prompt = \"In a futuristic city, robots and humans coexist peacefully, but\"\n",
        "\n",
        "# 4. Tokenize prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 5. Experiment with different temperature values\n",
        "temperature_values = [0.7, 1.0, 1.3]  # lower = conservative, higher = more creative\n",
        "\n",
        "for temp in temperature_values:\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=150,\n",
        "        do_sample=True,\n",
        "        temperature=temp,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"\\nGenerated Text with Temperature {temp}:\\n{generated_text}\\n{'-'*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KoP0JZXYrAd"
      },
      "source": [
        "**Q11.Implement a simple LSTM-based text generation model from scratch using\n",
        "Keras and train it on a custom data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgdouWOtYVB1",
        "outputId": "7a65d423-2bcb-4266-a416-43b78d9629e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: The cat is on the mat mat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# 1. Custom dataset of sentences\n",
        "sentences = [\n",
        "    \"The cat is on the mat\",\n",
        "    \"The dog is in the yard\",\n",
        "    \"Birds are flying in the sky\",\n",
        "    \"The sun rises in the east\",\n",
        "    \"The sun sets in the west\"\n",
        "]\n",
        "\n",
        "# 2. Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 3. Create input sequences (n-grams)\n",
        "input_sequences = []\n",
        "for line in sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# 4. Pad sequences\n",
        "max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# 5. Split features and labels\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# 6. Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50))  # embedding size 50\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train the model\n",
        "model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# 8. Generate text\n",
        "seed_text = \"The cat\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxe20vsZRLk"
      },
      "source": [
        "**Q12.How can you implement text generation using it in a simple custom\n",
        "attention-based architecture?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dphi69f8Y1gl",
        "outputId": "d38c39f5-ae17-41b8-e37e-1e9188618701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text with Attention: The cat is on the mat mat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# 1. Sample custom dataset\n",
        "sentences = [\n",
        "    \"The cat is on the mat\",\n",
        "    \"The dog is in the yard\",\n",
        "    \"Birds are flying in the sky\",\n",
        "    \"The sun rises in the east\",\n",
        "    \"The sun sets in the west\"\n",
        "]\n",
        "\n",
        "# 2. Tokenize sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 3. Create input sequences (n-grams)\n",
        "input_sequences = []\n",
        "for line in sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# 4. Pad sequences\n",
        "max_seq_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# 5. Split features and labels\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# 6. Define custom attention layer\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
        "                                 initializer=\"random_normal\", trainable=True)\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
        "                                 initializer=\"zeros\", trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "# 7. Build attention-based model\n",
        "inputs = Input(shape=(X.shape[1],))\n",
        "x = Embedding(total_words, 50)(inputs)\n",
        "lstm_out = LSTM(100, return_sequences=True)(x)\n",
        "att_out = Attention()(lstm_out)\n",
        "outputs = Dense(total_words, activation='softmax')(att_out)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 8. Train the model\n",
        "model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# 9. Generate text\n",
        "seed_text = \"The cat\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text with Attention:\", seed_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvlMs2OVTQhehH5j5XysN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}