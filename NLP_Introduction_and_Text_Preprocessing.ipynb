{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMngexQQOVESNA8Opn94DTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01anmolkumar/Assignment/blob/main/NLP_Introduction_and_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Introduction and Text Preprocessing"
      ],
      "metadata": {
        "id": "HuyR29uWa9gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is the primary goal of Natural Language Processing (NLP)?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* The primary goal of Natural Language Processing (NLP) is to enable computers to understand, interpret, and generate human language in a meaningful way.\n",
        "* NLP helps machines process and analyze large amounts of natural language data such as text and speech.\n",
        "* It aims to bridge the gap between human communication and computer understanding.\n",
        "\n",
        "* NLP allows computers to perform tasks like text classification, machine translation, sentiment analysis, and question answering.\n",
        "* It combines linguistics, computer science, and machine learning to extract meaning from language.\n",
        "* The ultimate objective of NLP is to make human–computer interaction more natural and efficient.\n"
      ],
      "metadata": {
        "id": "QdyJP-EvEejG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What does \"tokenization\" refer to in text processing?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Tokenization is the process of breaking text into smaller units called tokens.\n",
        "* These tokens may be words, sentences, or characters depending on the NLP task.\n",
        "* It is a fundamental step in text preprocessing in Natural Language Processing.\n",
        "\n",
        "* Tokenization helps convert unstructured text into a structured format that machines can understand.\n",
        "* It is used in tasks such as text classification, sentiment analysis, and information retrieval.\n",
        "* Proper tokenization improves the efficiency and accuracy of NLP models.\n"
      ],
      "metadata": {
        "id": "zY3ntKg_EhjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is the difference between lemmatization and stemming?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Lemmatization and stemming are text preprocessing techniques used to reduce words to their base form.\n",
        "* **Stemming** removes word endings using simple rules and may produce non-dictionary words.\n",
        "* **Lemmatization** reduces words to their meaningful base form called a lemma using vocabulary and grammar.\n",
        "\n",
        "* Stemming is faster and simpler but less accurate.\n",
        "* Lemmatization is slower but more accurate and preserves the correct meaning of words.\n",
        "* Stemming is often used when speed is important, while lemmatization is preferred for applications requiring linguistic accuracy.\n"
      ],
      "metadata": {
        "id": "BWwN4MYiFkk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the role of regular expressions (regex) in text processing?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Regular expressions (regex) are patterns used to search, match, and manipulate text.\n",
        "* They help identify specific text patterns such as emails, phone numbers, dates, or special characters.\n",
        "* Regex is widely used for cleaning and preprocessing text data.\n",
        "\n",
        "* In NLP, regex is used to remove unwanted characters, punctuation, or extra spaces.\n",
        "* It helps in extracting useful information from raw text.\n",
        "* Regex improves data quality and prepares text for further NLP tasks.\n"
      ],
      "metadata": {
        "id": "VCyxlO42Fv7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is Word2Vec and how does it represent words in a vector space?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word2Vec is a word embedding technique used in Natural Language Processing (NLP).\n",
        "* It represents words as dense numerical vectors in a continuous vector space.\n",
        "* Words with similar meanings are placed closer together in the vector space.\n",
        "* It is trained using neural network models on large text corpora.\n",
        "\n",
        "* There are two main Word2Vec models:\n",
        "  * CBOW (Continuous Bag of Words) – predicts a target word using surrounding context words.\n",
        "  * Skip-gram – predicts surrounding context words using a target word.\n",
        "\n",
        "* Word2Vec helps capture semantic relationships between words.\n",
        "* It improves performance in NLP tasks like text classification and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "t7zkgO1oFxON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. How does frequency distribution help in text analysis?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Frequency distribution shows how often each word appears in a given text or corpus.\n",
        "* It helps identify the most frequent and important words in a document.\n",
        "* Commonly occurring words can indicate the main topics or themes of the text.\n",
        "* It is useful for removing stopwords and noise during text preprocessing.\n",
        "* Frequency distribution supports tasks such as keyword extraction and text summarization.\n",
        "* It helps in feature selection for machine learning models in NLP.\n",
        "* Frequency analysis improves text classification and information retrieval tasks.\n",
        "* It provides statistical insight into the structure and patterns of textual data.\n"
      ],
      "metadata": {
        "id": "w7bsMtJHIRzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Why is text normalization important in NLP?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Text normalization is the process of converting text into a standard and consistent form.\n",
        "* It helps reduce variations in text such as uppercase/lowercase differences and punctuation.\n",
        "* Normalization improves the quality of text data before applying NLP models.\n",
        "* It helps handle spelling variations, contractions, and noisy text.\n",
        "* Text normalization reduces data sparsity and improves model accuracy.\n",
        "* It makes text easier to analyze and process by machine learning algorithms.\n",
        "* Common normalization techniques include lowercasing, removing punctuation, stopword removal, stemming, and lemmatization.\n"
      ],
      "metadata": {
        "id": "4zQPnWhsJIe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What is the difference between sentence tokenization and word tokenization?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Sentence tokenization is the process of dividing a text into individual sentences.\n",
        "* It helps identify sentence boundaries using punctuation like full stops and question marks.\n",
        "* Sentence tokenization is useful for tasks such as summarization and sentence-level analysis.\n",
        "\n",
        "* Word tokenization is the process of splitting sentences into individual words or tokens.\n",
        "* It helps analyze the structure and meaning of text at the word level.\n",
        "* Word tokenization is important for tasks like text classification, parsing, and word embeddings.\n"
      ],
      "metadata": {
        "id": "EgXIV_V6Jkfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What are co-occurrence vectors in NLP?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Co-occurrence vectors represent words based on how frequently they appear together in a given context.\n",
        "* They are built by counting the occurrence of neighboring words within a fixed window size.\n",
        "* Words that appear in similar contexts have similar co-occurrence vectors.\n",
        "* These vectors help capture semantic relationships between words.\n",
        "* Co-occurrence vectors are used to understand word similarity and context.\n",
        "* They form the basis for traditional word representation techniques in NLP.\n",
        "* Such vectors are useful in tasks like clustering, similarity measurement, and information retrieval.\n"
      ],
      "metadata": {
        "id": "lrhT4q0oKlIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What is the significance of lemmatization in improving NLP tasks?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Lemmatization converts words into their base or dictionary form.\n",
        "* It helps reduce multiple word forms into a single meaningful representation.\n",
        "* Lemmatization improves the consistency of text data.\n",
        "* It preserves the actual meaning of words compared to stemming.\n",
        "* Lemmatized text improves accuracy in tasks like search engines and chatbots.\n",
        "* It helps NLP models better understand language semantics.\n",
        "* Lemmatization reduces vocabulary size and improves model performance.\n"
      ],
      "metadata": {
        "id": "ShL6qjT6K2ZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.what is the Primary use of word embeddings in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word embeddings are dense vector representations of words in a continuous vector space.\n",
        "* They capture semantic and syntactic relationships between words, so that words with similar meanings are close together.\n",
        "* Embeddings allow NLP models to understand context and meaning in text more effectively than one-hot encoding.\n",
        "\n",
        "* Key features:\n",
        "  * **Semantic Similarity:** Words with similar meanings have vectors close to each other.\n",
        "  * **Contextual Understanding:** Helps models capture the meaning of words in different contexts.\n",
        "  * **Dimensionality Reduction:** Converts sparse representations like one-hot vectors into compact vectors.\n",
        "  * **Transfer Learning:** Pre-trained embeddings like Word2Vec, GloVe, and FastText can be used across different NLP tasks.\n",
        "\n",
        "* When to use:\n",
        "  * Use word embeddings when you need meaningful, dense numerical representations of words.\n",
        "  * Ideal for NLP tasks such as text classification, sentiment analysis, named entity recognition (NER), and machine translation.\n"
      ],
      "metadata": {
        "id": "e4fI_AUTRMvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. What is an annotator in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* An annotator in NLP is a component or tool that processes text to add structured information, such as labels, tags, or metadata.\n",
        "* It is used to enrich raw text with linguistic or semantic information for further analysis.\n",
        "\n",
        "* Key features:\n",
        "  * **Tokenization:** Breaks text into words, sentences, or tokens.\n",
        "  * **POS Tagging:** Assigns part-of-speech tags to words.\n",
        "  * **Named Entity Recognition (NER):** Identifies entities like names, dates, and locations.\n",
        "  * **Lemmatization:** Reduces words to their base or root form.\n",
        "  * **Dependency Parsing:** Analyzes grammatical structure and relationships between words.\n",
        "\n",
        "* When to use:\n",
        "  * Use an annotator when you need structured, machine-readable information from raw text.\n",
        "  * Essential for building NLP pipelines, training models, or performing text analytics tasks.\n"
      ],
      "metadata": {
        "id": "KjEOq_kEShfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. What are the key steps in text processing before applying machine learning models**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Text processing is essential to clean and structure raw text for machine learning models.\n",
        "* Key steps include:\n",
        "\n",
        "* **Key steps:**\n",
        "  * **Text Cleaning:** Removing special characters, numbers, punctuation, and irrelevant symbols.\n",
        "  * **Lowercasing:** Converting all text to lowercase for uniformity.\n",
        "  * **Tokenization:** Splitting text into words or sentences.\n",
        "  * **Stopword Removal:** Removing common words that do not contribute much meaning (e.g., \"the\", \"is\").\n",
        "  * **Stemming & Lemmatization:** Reducing words to their root or base form.\n",
        "  * **Text Normalization:** Converting text into a standard format (e.g., handling contractions, accents).\n",
        "  * **Vectorization:** Converting text into numerical representations like TF-IDF, Bag-of-Words, or embeddings.\n",
        "\n",
        "* When to use:\n",
        "  * Apply these steps before feeding text data into machine learning models for tasks like classification, sentiment analysis, or NLP predictions.\n"
      ],
      "metadata": {
        "id": "UfywPyj6S9vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. What is the history of NLP and how has it evolved**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Natural Language Processing (NLP) is the field of computer science and AI focused on the interaction between computers and human language.\n",
        "* Its history spans several decades and has evolved through multiple phases.\n",
        "\n",
        "* Key points in evolution:\n",
        "  * **1950s – 1960s:** Rule-based and symbolic approaches; early machine translation efforts.\n",
        "  * **1970s – 1980s:** Introduction of syntax-based and grammar-driven models.\n",
        "  * **1990s:** Statistical NLP emerged, using probabilistic models and corpora for tasks like tagging and parsing.\n",
        "  * **2000s:** Machine learning-based methods became dominant; use of supervised learning for classification and sequence labeling.\n",
        "  * **2010s – Present:** Deep learning and neural network-based NLP models (e.g., Word2Vec, BERT, GPT) achieved state-of-the-art performance.\n",
        "  * **Current trends:** Pre-trained language models, transfer learning, multilingual NLP, and contextual embeddings.\n",
        "\n",
        "* When to use:\n",
        "  * Understanding NLP history is useful to select the right approach and tools for research, production, or experimental tasks.\n"
      ],
      "metadata": {
        "id": "L5T4dg5HTCk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Why is sentence processing important in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Sentence processing is crucial in NLP because understanding text meaning often depends on the structure and context provided by sentences.\n",
        "* It enables machines to analyze relationships, syntax, and semantics at the sentence level.\n",
        "\n",
        "* Key points:\n",
        "  * **Syntactic Analysis:** Helps identify subjects, objects, and predicates in sentences.\n",
        "  * **Semantic Understanding:** Captures meaning beyond individual words by considering sentence context.\n",
        "  * **Coreference Resolution:** Tracks entities and references across sentences.\n",
        "  * **Sentiment Analysis:** Determines sentiment more accurately at sentence or phrase level.\n",
        "  * **Information Extraction:** Extracts relationships and facts from sentences for knowledge representation.\n",
        "\n",
        "* When to use:\n",
        "  * Use sentence processing for tasks like machine translation, question answering, summarization, and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "KthFqjKBTPha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. How do word embeddings improve the understanding of language semantics in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word embeddings are dense vector representations that capture semantic and syntactic meanings of words.\n",
        "* They allow models to understand relationships between words beyond simple text matching.\n",
        "\n",
        "* Key points:\n",
        "  * **Semantic Similarity:** Words with similar meanings have vectors close to each other (e.g., \"king\" and \"queen\").\n",
        "  * **Context Awareness:** Embeddings capture contextual relationships between words in sentences.\n",
        "  * **Dimensionality Reduction:** Converts sparse one-hot vectors into dense, meaningful vectors.\n",
        "  * **Transfer Learning:** Pre-trained embeddings like Word2Vec, GloVe, and FastText improve performance on downstream NLP tasks.\n",
        "\n",
        "* When to use:\n",
        "  * Use word embeddings to enhance machine learning models for text classification, NER, sentiment analysis, and machine translation.\n"
      ],
      "metadata": {
        "id": "jqxUxHwjTWMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. How does the frequency distribution of words help in text classification**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Frequency distribution counts the occurrence of each word in a text corpus.\n",
        "* It helps identify important words that contribute most to the meaning or category of the text.\n",
        "\n",
        "* Key points:\n",
        "  * **Feature Selection:** Frequent words can be used as features for classification.\n",
        "  * **Stopword Identification:** Extremely common words that carry little meaning can be removed.\n",
        "  * **Text Representation:** Frequency-based representations like Bag-of-Words or TF-IDF rely on word frequencies.\n",
        "  * **Topic Detection:** High-frequency words can indicate the main topics of the text.\n",
        "\n",
        "* When to use:\n",
        "  * Use frequency distribution to create features for text classification, spam detection, sentiment analysis, and topic modeling.\n"
      ],
      "metadata": {
        "id": "NB6kcBSCV5tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. What are the advantages of using regex in text cleaning**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Regular expressions (regex) are patterns used to match and manipulate text efficiently.\n",
        "* They are widely used in text preprocessing and cleaning for NLP tasks.\n",
        "\n",
        "* Key points:\n",
        "  * **Pattern Matching:** Identify specific patterns like emails, phone numbers, URLs, or dates.\n",
        "  * **Text Cleaning:** Remove unwanted characters, punctuation, or special symbols.\n",
        "  * **Flexibility:** Can handle a wide variety of text processing tasks with concise patterns.\n",
        "  * **Automation:** Enables batch processing of large text datasets without manual intervention.\n",
        "  * **Consistency:** Ensures uniform preprocessing across the dataset.\n",
        "\n",
        "* When to use:\n",
        "  * Use regex when you need precise, automated, and flexible text cleaning and pattern extraction.\n",
        "\n"
      ],
      "metadata": {
        "id": "m79Fj0A6WE-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. What is the difference between word2vec and doc2vec**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Word2Vec:** A model that generates vector representations for individual words based on their context in a corpus.\n",
        "* **Doc2Vec:** An extension of Word2Vec that generates vector representations for entire documents, paragraphs, or sentences.\n",
        "\n",
        "* Key differences:\n",
        "  * **Granularity:** Word2Vec → word-level embeddings; Doc2Vec → document-level embeddings.\n",
        "  * **Context Capture:** Doc2Vec captures the semantics of a whole document, not just individual words.\n",
        "  * **Applications:** Word2Vec → semantic similarity between words, word analogy tasks; Doc2Vec → document similarity, clustering, and classification.\n",
        "  * **Training:** Doc2Vec introduces a unique document ID vector to learn representations for each document along with word vectors.\n",
        "\n",
        "* When to use:\n",
        "  * Use Word2Vec for word-level semantic understanding.\n",
        "  * Use Doc2Vec when you need vector representations of entire documents or paragraphs for classification, clustering, or retrieval tasks.\n"
      ],
      "metadata": {
        "id": "61-sJb_mWzjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. Why is understanding text normalization important in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Text normalization is the process of converting text into a standard, consistent format.\n",
        "* It is important because raw text often contains variations, inconsistencies, and noise such as capitalization, punctuation, abbreviations, and special characters.\n",
        "\n",
        "* Key points:\n",
        "  * **Consistency:** Standardizes words, e.g., “won’t” → “will not”, “USA” → “usa”.\n",
        "  * **Noise Reduction:** Removes unwanted symbols, punctuation, and extra spaces.\n",
        "  * **Improves Model Accuracy:** Helps NLP models learn from clean, uniform input.\n",
        "  * **Facilitates Tokenization and Vectorization:** Makes downstream processing more effective.\n",
        "\n",
        "* When to use:\n",
        "  * Apply text normalization before tokenization, embedding generation, or any NLP task like classification, sentiment analysis, and information extraction.\n"
      ],
      "metadata": {
        "id": "EFFhekaXXECN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. How does word count help in text analysis**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word count is a basic text analysis technique that measures the frequency of each word in a document or corpus.\n",
        "* It provides insights into the importance and relevance of words within the text.\n",
        "\n",
        "* Key points:\n",
        "  * **Feature Extraction:** Frequent words can be used as features in machine learning models.\n",
        "  * **Identifying Key Terms:** Highlights important or recurring words that may indicate the topic.\n",
        "  * **Stopword Detection:** Extremely common words that do not add meaning can be removed.\n",
        "  * **Text Summarization:** Helps identify keywords and significant content.\n",
        "  * **Trend Analysis:** Useful in social media, customer reviews, or surveys to detect common themes.\n",
        "\n",
        "* When to use:\n",
        "  * Use word count as an initial step in exploratory text analysis, feature engineering, and preparing data for NLP models like classification or clustering.\n"
      ],
      "metadata": {
        "id": "J69T72BaXe_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. How does lemmatization help in NLP tasks like search engines and chatbots**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Lemmatization is the process of reducing words to their base or root form (lemma) while preserving their meaning.\n",
        "* It helps NLP systems understand different forms of a word as the same concept.\n",
        "\n",
        "* Key points:\n",
        "  * **Improves Search Accuracy:** “running”, “ran”, and “runs” are all mapped to “run”, enabling better search results.\n",
        "  * **Reduces Vocabulary Size:** Simplifies the text representation for machine learning models.\n",
        "  * **Enhances Understanding:** Helps chatbots and virtual assistants interpret user queries more effectively.\n",
        "  * **Consistent Features:** Ensures that variations of a word are treated as a single feature in NLP tasks.\n",
        "  * **Supports Text Analysis:** Facilitates sentiment analysis, information retrieval, and document classification.\n",
        "\n",
        "* When to use:\n",
        "  * Apply lemmatization during preprocessing in tasks like search engines, question-answering systems, chatbots, and text classification to improve accuracy and reduce noise.\n"
      ],
      "metadata": {
        "id": "K0cVdbDBXtKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. What is the purpose of using Doc2Vec in text processing**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Doc2Vec is an extension of Word2Vec that generates vector representations for entire documents, paragraphs, or sentences instead of individual words.\n",
        "* Its purpose is to capture the semantic meaning of a larger text unit for NLP tasks.\n",
        "\n",
        "* Key points:\n",
        "  * **Document-Level Semantics:** Represents the overall meaning of a document, not just individual words.\n",
        "  * **Text Similarity:** Helps measure similarity between documents or paragraphs.\n",
        "  * **Feature Extraction:** Produces dense vectors that can be used as input for machine learning models.\n",
        "  * **Classification and Clustering:** Useful in document classification, recommendation systems, and clustering tasks.\n",
        "  * **Context Preservation:** Captures the context of words within the document.\n",
        "\n",
        "* When to use:\n",
        "  * Use Doc2Vec when you need embeddings for entire documents or paragraphs for classification, clustering, or semantic search tasks.\n"
      ],
      "metadata": {
        "id": "AsPazK4vZD_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24. What is the importance of sentence processing in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Sentence processing involves analyzing and understanding text at the sentence level rather than just individual words.\n",
        "* It is important because the meaning of text often depends on sentence structure and context.\n",
        "\n",
        "* Key points:\n",
        "  * **Syntactic Understanding:** Identifies subjects, verbs, objects, and other grammatical elements.\n",
        "  * **Semantic Analysis:** Captures meaning by considering relationships between words within a sentence.\n",
        "  * **Coreference Resolution:** Tracks entities and references across sentences.\n",
        "  * **Sentiment Analysis:** Determines sentiment more accurately by analyzing sentence-level context.\n",
        "  * **Information Extraction:** Extracts relationships and facts from sentences for knowledge representation.\n",
        "\n",
        "* When to use:\n",
        "  * Apply sentence processing for machine translation, summarization, question answering, sentiment analysis, and information extraction tasks.\n"
      ],
      "metadata": {
        "id": "MjHaSGyHZ5qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25. What is text normalization, and what are the common techniques used in it**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Text normalization is the process of converting text into a consistent, standard format to reduce variability and noise.\n",
        "* It ensures that NLP models can interpret text accurately regardless of variations in spelling, punctuation, or style.\n",
        "\n",
        "* Common techniques:\n",
        "  * **Lowercasing:** Converting all text to lowercase for uniformity.\n",
        "  * **Removing Punctuation & Special Characters:** Cleans the text for analysis.\n",
        "  * **Expanding Contractions:** Converts “don’t” to “do not” and similar forms.\n",
        "  * **Handling Accents and Unicode:** Standardizes characters across languages.\n",
        "  * **Spelling Correction:** Fixes common typos or variations.\n",
        "  * **Removing Stopwords:** Eliminates common words that add little meaning.\n",
        "\n",
        "* When to use:\n",
        "  * Apply text normalization during preprocessing before tokenization, vectorization, or feeding data into NLP models for classification, sentiment analysis, or other tasks.\n"
      ],
      "metadata": {
        "id": "UdhcTGLzZ9z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26. Why is word tokenization important in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word tokenization is the process of breaking text into individual words or tokens.\n",
        "* It is a fundamental step in NLP because most models and algorithms operate on word-level input.\n",
        "\n",
        "* Key points:\n",
        "  * **Prepares Text for Analysis:** Converts raw text into manageable units for further processing.\n",
        "  * **Enables Feature Extraction:** Tokens are used for creating features like Bag-of-Words, TF-IDF, or embeddings.\n",
        "  * **Supports Downstream NLP Tasks:** Essential for tasks like part-of-speech tagging, named entity recognition, and sentiment analysis.\n",
        "  * **Improves Context Understanding:** Helps models understand word usage in different contexts.\n",
        "\n",
        "* When to use:\n",
        "  * Use word tokenization as the first step in preprocessing text for machine learning and NLP tasks.\n"
      ],
      "metadata": {
        "id": "nGvEFuHoaCN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**27. How does sentence tokenization differ from word tokenization in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Sentence tokenization is the process of splitting text into individual sentences, while word tokenization splits text into individual words or tokens.\n",
        "* Both are essential preprocessing steps, but they operate at different levels of granularity.\n",
        "\n",
        "* Key points:\n",
        "  * **Sentence Tokenization:** Helps understand the structure and meaning of text at the sentence level.\n",
        "  * **Word Tokenization:** Breaks sentences into words for detailed analysis and feature extraction.\n",
        "  * **Use in Context:** Sentence tokenization is often used in summarization, translation, and sentiment analysis; word tokenization is used for embedding, classification, and language modeling.\n",
        "  * **Hierarchical Processing:** Sentence tokenization is typically performed before word tokenization for better structure and context handling.\n",
        "\n",
        "* When to use:\n",
        "  * Use sentence tokenization when sentence-level analysis is required, and word tokenization when word-level processing or modeling is needed.\n"
      ],
      "metadata": {
        "id": "jmoEqLfcaF5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**28. What is the primary purpose of text processing in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* The primary purpose of text processing in NLP is to clean, structure, and transform raw text into a format that can be effectively analyzed and used by machine learning models.\n",
        "* It ensures that textual data is consistent, meaningful, and suitable for computational analysis.\n",
        "\n",
        "* Key points:\n",
        "  * **Noise Reduction:** Removes irrelevant characters, punctuation, and inconsistencies.\n",
        "  * **Standardization:** Converts text into a uniform format through normalization, lowercasing, and tokenization.\n",
        "  * **Feature Preparation:** Prepares data for feature extraction methods like TF-IDF, Bag-of-Words, or embeddings.\n",
        "  * **Improves Model Accuracy:** Enhances the performance of NLP models by providing clean and structured input.\n",
        "  * **Supports Downstream Tasks:** Essential for classification, sentiment analysis, information retrieval, and machine translation.\n",
        "\n",
        "* When to use:\n",
        "  * Apply text processing as a foundational step before any NLP modeling, analysis, or text-based machine learning task.\n"
      ],
      "metadata": {
        "id": "y7p9AzMIaNnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29. What are the key challenges in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* NLP faces several challenges due to the complexity, variability, and ambiguity of human language.\n",
        "* Understanding and processing natural language accurately requires overcoming these obstacles.\n",
        "\n",
        "* Key challenges:\n",
        "  * **Ambiguity:** Words and sentences can have multiple meanings depending on context.\n",
        "  * **Sarcasm and Irony:** Difficult for models to detect sentiment accurately.\n",
        "  * **Polysemy and Homonyms:** Words with multiple meanings create confusion.\n",
        "  * **Context Understanding:** Capturing long-range dependencies in text.\n",
        "  * **Language Variability:** Slang, dialects, and informal writing styles.\n",
        "  * **Resource Limitations:** Lack of labeled data for certain languages or domains.\n",
        "  * **Syntax and Grammar Variations:** Complex sentence structures can be hard to parse.\n",
        "  * **Cross-lingual NLP:** Differences in grammar, syntax, and semantics across languages.\n",
        "\n",
        "* When to use:\n",
        "  * Being aware of these challenges helps in selecting appropriate preprocessing techniques, models, and evaluation strategies for NLP projects.\n"
      ],
      "metadata": {
        "id": "GQMyxFoXaRt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**30. How do co-occurrence vectors represent relationships between words**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Co-occurrence vectors represent words based on how frequently they appear together with other words in a text corpus.\n",
        "* They capture semantic and contextual relationships between words.\n",
        "\n",
        "* Key points:\n",
        "  * **Context Representation:** Words that appear in similar contexts have similar co-occurrence patterns.\n",
        "  * **Semantic Similarity:** Helps identify words with related meanings based on shared context.\n",
        "  * **Feature Creation:** Used in models like Word2Vec, GloVe, and other vector space representations.\n",
        "  * **Dimensionality Reduction:** Co-occurrence matrices can be transformed into dense embeddings for machine learning.\n",
        "\n",
        "* When to use:\n",
        "  * Use co-occurrence vectors to understand word relationships, build embeddings, and improve performance in NLP tasks like similarity analysis, clustering, and classification.\n"
      ],
      "metadata": {
        "id": "udjkd2aZaU25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**31. What is the role of frequency distribution in text analysis**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Frequency distribution measures how often each word or token appears in a text corpus.\n",
        "* It provides insights into the importance, relevance, and patterns of words within the text.\n",
        "\n",
        "* Key points:\n",
        "  * **Feature Extraction:** Frequent words can be used as features in NLP models.\n",
        "  * **Keyword Identification:** Highlights important terms that indicate the topic or theme.\n",
        "  * **Stopword Detection:** Very common words that add little meaning can be filtered out.\n",
        "  * **Text Summarization:** Helps identify key phrases and significant content.\n",
        "  * **Trend Analysis:** Useful for analyzing word usage over time or across documents.\n",
        "\n",
        "* When to use:\n",
        "  * Use frequency distribution in exploratory text analysis, feature engineering, and preparing datasets for classification, clustering, or topic modeling.\n"
      ],
      "metadata": {
        "id": "ipYJaofLaZNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**32. What is the impact of word embeddings on NLP tasks**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word embeddings are dense vector representations of words that capture semantic and syntactic relationships.\n",
        "* They have significantly improved the performance of many NLP tasks by providing meaningful numerical representations of text.\n",
        "\n",
        "* Key points:\n",
        "  * **Semantic Understanding:** Words with similar meanings are close in vector space, aiding tasks like similarity detection.\n",
        "  * **Context Awareness:** Embeddings capture relationships between words in different contexts.\n",
        "  * **Dimensionality Reduction:** Converts sparse representations like one-hot encoding into compact, efficient vectors.\n",
        "  * **Improved Model Performance:** Enhances classification, sentiment analysis, named entity recognition, machine translation, and other NLP tasks.\n",
        "  * **Transfer Learning:** Pre-trained embeddings (Word2Vec, GloVe, FastText) can be used across different tasks, reducing training time and data requirements.\n",
        "\n",
        "* When to use:\n",
        "  * Use word embeddings to provide richer, context-aware input to NLP models and improve task accuracy and efficiency.\n"
      ],
      "metadata": {
        "id": "FzpNY1Nyacdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**33. What is the purpose of using lemmatization in text preprocessing**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Lemmatization reduces words to their base or root form (lemma) while preserving their meaning.\n",
        "* It is used to standardize text and reduce variability in word forms for NLP tasks.\n",
        "\n",
        "* Key points:\n",
        "  * **Consistency:** Treats different forms of a word (e.g., “running”, “ran”, “runs”) as the same token (“run”).\n",
        "  * **Reduces Vocabulary Size:** Simplifies the text representation for machine learning models.\n",
        "  * **Improves Accuracy:** Helps models better understand the context and meaning of words.\n",
        "  * **Supports Downstream NLP Tasks:** Useful in search engines, sentiment analysis, text classification, and chatbots.\n",
        "\n",
        "* When to use:\n",
        "  * Apply lemmatization during preprocessing to normalize text, reduce noise, and improve the performance of NLP models.\n"
      ],
      "metadata": {
        "id": "U-HVp1I9afug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "SkebVfGEa50k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.How can you perform word tokenization using NLTK**"
      ],
      "metadata": {
        "id": "a2u17d6-cXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # Download tokenizer models, specifically 'punkt_tab' as suggested by the error\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfxRjTA5K0Jh",
        "outputId": "12aa3274-97c2-4495-82c2-84ed5f62eb42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.How can you perform sentence tokenization using NLTK**"
      ],
      "metadata": {
        "id": "RBqYaptCc4Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download tokenizer models\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"NLTK is a leading platform for building Python programs. It helps in working with human language data.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ZfzsSMcw64",
        "outputId": "6018e9b5-0485-4a95-9c55-fb6db17c9647"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK is a leading platform for building Python programs.', 'It helps in working with human language data.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.How can you remove stopwords from a sentence**"
      ],
      "metadata": {
        "id": "LhTkIa3PdO3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMXMFeSKdFx4",
        "outputId": "d5ffee0e-6bab-4570-b325-dca9b0ca1cb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4How can you perform stemming on a word\u001c**"
      ],
      "metadata": {
        "id": "HHu4_xq4d910"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK is helping in building programs that process human languages.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(w) for w in words]\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DXYR5d9dUNi",
        "outputId": "9d696afb-ebd7-480a-b658-b0f667777a98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nltk', 'is', 'help', 'in', 'build', 'program', 'that', 'process', 'human', 'languag', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.How can you perform lemmatization on a word\u001c**"
      ],
      "metadata": {
        "id": "N3-6aNpaeIi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in words]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2EEq4B2eE0Y",
        "outputId": "378c24dc-6060-43f8-e143-95e194098637"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Q6.How can you normalize a text by converting it to lowercase and removing punctuation\u001c**"
      ],
      "metadata": {
        "id": "CSzGJVC7eSgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"NLTK is a leading platform for building Python programs!\"\n",
        "# Convert to lowercase and remove punctuation\n",
        "normalized_text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOLwYNnWePAZ",
        "outputId": "7b39bca7-ce06-456c-e533-a6b05b8e055f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk is a leading platform for building python programs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7How can you create a co-occurrence matrix for words in a corpus\u001c**"
      ],
      "metadata": {
        "id": "t5_gE6Age_2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    \"NLTK is great for NLP tasks\",\n",
        "    \"Python and NLTK are popular\",\n",
        "    \"NLP tasks can be done with Python\"\n",
        "]\n",
        "\n",
        "# Convert text to a bag-of-words matrix\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute co-occurrence matrix\n",
        "co_occurrence = (X.T * X)  # Matrix multiplication\n",
        "co_occurrence.setdiag(0)   # Set diagonal to 0\n",
        "\n",
        "df = pd.DataFrame(co_occurrence.toarray(),\n",
        "                  index=vectorizer.get_feature_names_out(),\n",
        "                  columns=vectorizer.get_feature_names_out())\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU6GvQqXefJf",
        "outputId": "efff9a28-7561-484d-9fa0-2fbf7230bf6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         and  are  be  can  done  for  great  is  nlp  nltk  popular  python  \\\n",
            "and        0    1   0    0     0    0      0   0    0     1        1       1   \n",
            "are        1    0   0    0     0    0      0   0    0     1        1       1   \n",
            "be         0    0   0    1     1    0      0   0    1     0        0       1   \n",
            "can        0    0   1    0     1    0      0   0    1     0        0       1   \n",
            "done       0    0   1    1     0    0      0   0    1     0        0       1   \n",
            "for        0    0   0    0     0    0      1   1    1     1        0       0   \n",
            "great      0    0   0    0     0    1      0   1    1     1        0       0   \n",
            "is         0    0   0    0     0    1      1   0    1     1        0       0   \n",
            "nlp        0    0   1    1     1    1      1   1    0     1        0       1   \n",
            "nltk       1    1   0    0     0    1      1   1    1     0        1       1   \n",
            "popular    1    1   0    0     0    0      0   0    0     1        0       1   \n",
            "python     1    1   1    1     1    0      0   0    1     1        1       0   \n",
            "tasks      0    0   1    1     1    1      1   1    2     1        0       1   \n",
            "with       0    0   1    1     1    0      0   0    1     0        0       1   \n",
            "\n",
            "         tasks  with  \n",
            "and          0     0  \n",
            "are          0     0  \n",
            "be           1     1  \n",
            "can          1     1  \n",
            "done         1     1  \n",
            "for          1     0  \n",
            "great        1     0  \n",
            "is           1     0  \n",
            "nlp          2     1  \n",
            "nltk         1     0  \n",
            "popular      0     0  \n",
            "python       1     1  \n",
            "tasks        0     1  \n",
            "with         1     0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.How can you apply a regular expression to extract all email addresses from a text**"
      ],
      "metadata": {
        "id": "d_-E0DEQfu12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"For inquiries, contact us at support@example.com or sales@domain.org.\"\n",
        "\n",
        "# Regular expression pattern for email addresses\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "# Find all email addresses in the text\n",
        "emails = re.findall(email_pattern, text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUB1e8k5fG7x",
        "outputId": "8cfed885-70d4-4eba-acec-fd99e50b3b6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'sales@domain.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.How can you perform word embedding using Word2Vec**\n"
      ],
      "metadata": {
        "id": "-Jrdf7WSg3k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"NLTK is great for natural language processing\",\n",
        "    \"Python and NLTK are widely used in NLP\",\n",
        "    \"Word2Vec converts words into vectors\"\n",
        "]\n",
        "\n",
        "# Tokenize each sentence\n",
        "sentences = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Get vector for a word\n",
        "vector_nltk = model.wv['nltk']\n",
        "print(vector_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtZzYV6xf_V6",
        "outputId": "3cd193e0-201d-4b87-f637-e225c9e03445"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "[-1.07255566e-03  4.72907297e-04  1.02076605e-02  1.80202425e-02\n",
            " -1.86076518e-02 -1.42349601e-02  1.29189622e-02  1.79476682e-02\n",
            " -1.00318016e-02 -7.52745243e-03  1.47624006e-02 -3.06723197e-03\n",
            " -9.07408167e-03  1.31093385e-02 -9.72123723e-03 -3.63237783e-03\n",
            "  5.75370155e-03  1.98393455e-03 -1.65719930e-02 -1.88994166e-02\n",
            "  1.46249095e-02  1.01414807e-02  1.35166608e-02  1.52587483e-03\n",
            "  1.27029773e-02 -6.81137387e-03 -1.89298124e-03  1.15382336e-02\n",
            " -1.50446929e-02 -7.87294935e-03 -1.50245801e-02 -1.86025980e-03\n",
            "  1.90780368e-02 -1.46397129e-02 -4.66797687e-03 -3.87584744e-03\n",
            "  1.61563959e-02 -1.18629094e-02  9.03333930e-05 -9.50836390e-03\n",
            " -1.92089099e-02  1.00155296e-02 -1.75208207e-02 -8.78447853e-03\n",
            " -7.02065809e-05 -5.92418713e-04 -1.53239239e-02  1.92312989e-02\n",
            "  9.96505469e-03  1.84680279e-02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.How can you use Doc2Vec to embed documents**"
      ],
      "metadata": {
        "id": "6h5R2Ne4h6Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"NLTK is great for natural language processing\",\n",
        "    \"Python and NLTK are widely used in NLP\",\n",
        "    \"Doc2Vec converts entire documents into vectors\"\n",
        "]\n",
        "\n",
        "# Tag each document\n",
        "tagged_docs = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)])\n",
        "               for i, doc in enumerate(documents)]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(tagged_docs, vector_size=50, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Get vector for first document\n",
        "vector_doc0 = model.dv['0']\n",
        "print(vector_doc0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxnusCDkhN-q",
        "outputId": "30527c98-529e-4209-f62d-6f8c048aea74"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01051425 -0.01200803 -0.01984484  0.01718797  0.00715555  0.0005316\n",
            " -0.01983877 -0.01036656 -0.01953026  0.00404406  0.00569302  0.00932899\n",
            " -0.00862388 -0.00632032 -0.00617726 -0.01751844  0.00436862  0.01852817\n",
            " -0.01909333 -0.00696244 -0.00757191  0.00524764 -0.01143321  0.00524516\n",
            "  0.0116477  -0.0162785  -0.0167438  -0.01999676  0.00990463 -0.01832371\n",
            "  0.01174335  0.01366131 -0.01307067 -0.00907582 -0.00251721  0.00331046\n",
            " -0.00296935 -0.01715733 -0.0072277   0.00347706 -0.00413607 -0.01450941\n",
            "  0.00838662 -0.01723282  0.00544357 -0.00925679  0.00129289 -0.00412469\n",
            "  0.01088897 -0.01606476]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.How can you perform part-of-speech tagging**"
      ],
      "metadata": {
        "id": "TFYaWwY4iIbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"NLTK is great for natural language processing\",\n",
        "    \"Python and NLTK are widely used in NLP\",\n",
        "    \"Doc2Vec converts entire documents into vectors\"\n",
        "]\n",
        "\n",
        "# Tag each document\n",
        "tagged_docs = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)])\n",
        "               for i, doc in enumerate(documents)]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(tagged_docs, vector_size=50, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Get vector for first document\n",
        "vector_doc0 = model.dv['0']\n",
        "print(vector_doc0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpQGwyACin5P",
        "outputId": "ca6481b5-e118-476c-9d5f-55dc4a18fbcc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01051425 -0.01200803 -0.01984484  0.01718797  0.00715555  0.0005316\n",
            " -0.01983877 -0.01036656 -0.01953026  0.00404406  0.00569302  0.00932899\n",
            " -0.00862388 -0.00632032 -0.00617726 -0.01751844  0.00436862  0.01852817\n",
            " -0.01909333 -0.00696244 -0.00757191  0.00524764 -0.01143321  0.00524516\n",
            "  0.0116477  -0.0162785  -0.0167438  -0.01999676  0.00990463 -0.01832371\n",
            "  0.01174335  0.01366131 -0.01307067 -0.00907582 -0.00251721  0.00331046\n",
            " -0.00296935 -0.01715733 -0.0072277   0.00347706 -0.00413607 -0.01450941\n",
            "  0.00838662 -0.01723282  0.00544357 -0.00925679  0.00129289 -0.00412469\n",
            "  0.01088897 -0.01606476]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.How can you find the similarity between two sentences using cosine similarity**"
      ],
      "metadata": {
        "id": "UQbBEAgAklgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Two example sentences\n",
        "sentence1 = \"NLTK is used for NLP tasks\"\n",
        "sentence2 = \"Python has NLTK for natural language processing\"\n",
        "\n",
        "# Convert sentences into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([sentence1, sentence2])\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(vectors[0:1], vectors[1:2])\n",
        "print(similarity[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_4Z3l9Aip5a",
        "outputId": "61efa41f-fa40-43dc-be17-1b6eaa07155c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18443191662261307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13How can you extract named entities from a sentence**"
      ],
      "metadata": {
        "id": "5ss-Djg2lcri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Changed to download the specific English tagger\n",
        "nltk.download('maxent_ne_chunker_tab') # Added to download the specific resource for NE chunking\n",
        "\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "\n",
        "sentence = \"Apple is looking at buying a U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "# Named Entity Recognition\n",
        "entities = ne_chunk(tags)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVPWLY4qk_B8",
        "outputId": "0a806d4b-33de-4589-f84b-02cba82380e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  a/DT\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14How can you split a large document into smaller chunks of text\u001c**"
      ],
      "metadata": {
        "id": "u_t1FnPcluLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=50):\n",
        "    \"\"\"\n",
        "    Splits a text into smaller chunks of words.\n",
        "\n",
        "    :param text: The input text (string)\n",
        "    :param chunk_size: Number of words per chunk\n",
        "    :return: List of text chunks\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "large_text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\"\n",
        "chunks = chunk_text(large_text, chunk_size=10)\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6abKJrPlj3b",
        "outputId": "78f9091f-8b56-47b2-b961-10aa6dd6fd05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK is a leading platform for building Python programs to', 'work with human language data. It provides easy-to-use interfaces to', 'over 50 corpora and lexical resources.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of documents\u001c**"
      ],
      "metadata": {
        "id": "T_bE9enKmYeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"NLTK is great for natural language processing\",\n",
        "    \"Python and NLTK are widely used in NLP\",\n",
        "    \"TF-IDF converts text into numerical vectors\"\n",
        "]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1G6ZHtumKJ2",
        "outputId": "c5b81ac2-f49e-4e82-8a37-322026fbb7a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        and       are  converts       for     great       idf        in  \\\n",
            "0  0.000000  0.000000  0.000000  0.389888  0.389888  0.000000  0.000000   \n",
            "1  0.363255  0.363255  0.000000  0.000000  0.000000  0.000000  0.363255   \n",
            "2  0.000000  0.000000  0.377964  0.000000  0.000000  0.377964  0.000000   \n",
            "\n",
            "       into        is  language  ...       nlp      nltk  numerical  \\\n",
            "0  0.000000  0.389888  0.389888  ...  0.000000  0.296520   0.000000   \n",
            "1  0.000000  0.000000  0.000000  ...  0.363255  0.276265   0.000000   \n",
            "2  0.377964  0.000000  0.000000  ...  0.000000  0.000000   0.377964   \n",
            "\n",
            "   processing    python      text        tf      used   vectors    widely  \n",
            "0    0.389888  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1    0.000000  0.363255  0.000000  0.000000  0.363255  0.000000  0.363255  \n",
            "2    0.000000  0.000000  0.377964  0.377964  0.000000  0.377964  0.000000  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16How can you apply tokenization, stopword removal, and stemming in one go**"
      ],
      "metadata": {
        "id": "lMu0zvoQmjjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "# Initialize stemmer and stopwords\n",
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Process text: tokenize, remove stopwords, and stem\n",
        "processed_words = [ps.stem(w) for w in word_tokenize(text) if w.lower() not in stop_words]\n",
        "print(processed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYMi-Lzdmoyw",
        "outputId": "d5f02f21-e02f-4147-e402-19fd6b780e36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17How can you visualize the frequency distribution of words in a sentence?**"
      ],
      "metadata": {
        "id": "OvFE1ogomuO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"NLTK is a leading platform for building Python programs. NLTK provides tools for NLP tasks.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text.lower())\n",
        "\n",
        "# Compute frequency distribution\n",
        "fdist = FreqDist(words)\n",
        "\n",
        "# Plot the top 10 most frequent words\n",
        "fdist.plot(10, cumulative=False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "uEiUZzHNmew0",
        "outputId": "a6eeab87-39a1-42a0-b712-a80e9c2c572a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AONFqTvgm2KR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}