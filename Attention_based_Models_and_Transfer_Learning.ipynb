{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlMtgy4FGlwnVR442g/yK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01anmolkumar/Assignment/blob/main/Attention_based_Models_and_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention based Models and Transfer Learning"
      ],
      "metadata": {
        "id": "UCZ7rlAWdaJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is BERT and how does it work**\n",
        "\n",
        "* BERT stands for Bidirectional Encoder Representations from Transformers.\n",
        "* It is a pre-trained language model developed by Google.\n",
        "* BERT is based on the Transformer encoder architecture.\n",
        "* It reads text bidirectionally, meaning it considers both left and right context.\n",
        "* This allows BERT to understand the full meaning of words in a sentence.\n",
        "* It is trained using Masked Language Modeling (MLM).\n",
        "* In MLM, some words are masked and the model predicts them.\n",
        "* BERT is also trained using Next Sentence Prediction (NSP).\n",
        "* It is widely used for tasks like classification, NER, QA, and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "VG_MrcrzZ5Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the main advantages of using the attention mechanism in neural networks**\n",
        "\n",
        "* The attention mechanism allows models to focus on the most relevant parts of the input.\n",
        "* It improves understanding of context within long sequences.\n",
        "* Attention helps handle long-range dependencies effectively.\n",
        "* It reduces information loss compared to fixed-size context vectors.\n",
        "* Models with attention can process sequences in parallel.\n",
        "* It improves performance in tasks like translation and text summarization.\n",
        "* Attention provides better interpretability by showing what the model focuses on.\n",
        "* It enhances overall accuracy and efficiency in neural network models.\n"
      ],
      "metadata": {
        "id": "HqlWlnnOZ_YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How does the self-attention mechanism differ from traditional attention mechanisms**\n",
        "\n",
        "* Self-attention operates within a single sequence to relate different positions.\n",
        "* It allows each word to attend to every other word in the same sentence.\n",
        "* Traditional attention typically connects encoder outputs with decoder states.\n",
        "* Self-attention does not require separate encoder-decoder alignment.\n",
        "* It captures contextual relationships more effectively.\n",
        "* Self-attention enables parallel computation across the sequence.\n",
        "* It reduces dependency on sequential processing.\n",
        "* This results in faster training and better representation learning.\n"
      ],
      "metadata": {
        "id": "fbOLW9SuaHUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the role of the decoder in a Seq2Seq model**\n",
        "\n",
        "* The decoder generates the output sequence step by step.\n",
        "* It takes the context vector from the encoder as input.\n",
        "* The decoder predicts one token at a time.\n",
        "* Each prediction depends on previous generated tokens.\n",
        "* It uses hidden states to maintain sequence context.\n",
        "* Attention (if used) helps focus on relevant encoder outputs.\n",
        "* The decoder converts encoded information into meaningful output.\n",
        "* It is essential for tasks like translation and text generation.\n"
      ],
      "metadata": {
        "id": "p75fLdzyaSZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is the difference between GPT-2 and BERT models**\n",
        "\n",
        "* GPT-2 is an autoregressive language model.\n",
        "* It generates text from left to right.\n",
        "* GPT-2 is based on the Transformer decoder architecture.\n",
        "* BERT is a bidirectional language model.\n",
        "* It uses the Transformer encoder architecture.\n",
        "* BERT looks at both left and right context simultaneously.\n",
        "* GPT-2 is mainly used for text generation tasks.\n",
        "* BERT is commonly used for understanding tasks like classification and NER.\n"
      ],
      "metadata": {
        "id": "QKVXD6lyacBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Why is the Transformer model considered more efficient than RNNs and LSTMs**\n",
        "\n",
        "* Transformers process entire sequences in parallel.\n",
        "* RNNs and LSTMs process data sequentially.\n",
        "* Parallelism significantly reduces training time.\n",
        "* Transformers use self-attention instead of recurrence.\n",
        "* This helps capture long-range dependencies efficiently.\n",
        "* There is no vanishing gradient problem due to long sequences.\n",
        "* Transformers scale better with large datasets.\n",
        "* They achieve higher performance on many NLP tasks.\n"
      ],
      "metadata": {
        "id": "-Xhv95uValg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Explain how the attention mechanism works in a Transformer model**\n",
        "\n",
        "* The Transformer uses self-attention to relate all words in a sequence.\n",
        "* Each word is converted into query, key, and value vectors.\n",
        "* Attention scores are computed using dot products of queries and keys.\n",
        "* Scores are scaled and passed through a softmax function.\n",
        "* The softmax output determines the importance of each word.\n",
        "* Values are weighted based on attention scores.\n",
        "* The weighted values are summed to produce the output.\n",
        "* This allows the model to capture contextual relationships effectively.\n"
      ],
      "metadata": {
        "id": "vZans9kraqJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What is the difference between an encoder and a decoder in a Seq2Seq model**\n",
        "\n",
        "* The encoder processes the input sequence.\n",
        "* It converts the input into a fixed-length or contextual representation.\n",
        "* The decoder generates the output sequence.\n",
        "* It uses the encoder’s output as context.\n",
        "* The encoder focuses on understanding input data.\n",
        "* The decoder focuses on producing meaningful output.\n",
        "* Encoders work on the full input sequence at once.\n",
        "* Decoders generate outputs step by step.\n"
      ],
      "metadata": {
        "id": "MYBqxS7dau1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What is the primary purpose of using the self-attention mechanism in transformers**\n",
        "\n",
        "* Self-attention helps the model understand relationships between words.\n",
        "* It allows each token to attend to all other tokens in the sequence.\n",
        "* This provides better contextual understanding.\n",
        "* It captures long-range dependencies efficiently.\n",
        "* Self-attention removes the need for recurrence.\n",
        "* It enables parallel processing of sequences.\n",
        "* This improves training speed and scalability.\n",
        "* It enhances overall model performance in NLP tasks.\n"
      ],
      "metadata": {
        "id": "EmS1HZ9hay6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. How does the GPT-2 model generate text**\n",
        "\n",
        "* GPT-2 generates text in an autoregressive manner.\n",
        "* It predicts the next word based on previous words.\n",
        "* The model processes input from left to right.\n",
        "* It uses a Transformer decoder architecture.\n",
        "* Each generated token is fed back as input.\n",
        "* Probabilities are computed using a softmax layer.\n",
        "* The word with the highest or sampled probability is selected.\n",
        "* This process continues until the sequence is complete.\n"
      ],
      "metadata": {
        "id": "8Z2v9xXfa3FK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. What is the main difference between the encoder-decoder architecture and a simple neural network**\n",
        "\n",
        "* An encoder-decoder architecture has two separate components.\n",
        "* The encoder processes and understands the input sequence.\n",
        "* The decoder generates the output sequence based on encoder output.\n",
        "* A simple neural network has a single processing flow.\n",
        "* Encoder-decoder models handle variable-length input and output.\n",
        "* Simple neural networks usually work with fixed-size inputs.\n",
        "* Encoder-decoder architecture is suitable for sequence tasks.\n",
        "* It is widely used in translation and text generation problems.\n"
      ],
      "metadata": {
        "id": "8b9yVQHfa7SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. Explain the concept of “fine-tuning” in BERT**\n",
        "\n",
        "* Fine-tuning is the process of adapting a pre-trained BERT model.\n",
        "* It involves training the model on a specific downstream task.\n",
        "* The pre-trained weights are used as a starting point.\n",
        "* A task-specific layer is added on top of BERT.\n",
        "* The model is trained with labeled task data.\n",
        "* Fine-tuning requires less data compared to training from scratch.\n",
        "* It improves task-specific performance.\n",
        "* Fine-tuning makes BERT flexible for many NLP applications.\n"
      ],
      "metadata": {
        "id": "VfayCfAFa_1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13. How does the attention mechanism handle long-range dependencies in sequences**\n",
        "\n",
        "* Attention allows each token to connect with all other tokens.\n",
        "* It directly models relationships regardless of distance.\n",
        "* Unlike RNNs, it does not rely on sequential steps.\n",
        "* This prevents information loss over long sequences.\n",
        "* Long-range dependencies are captured in a single layer.\n",
        "* Attention weights highlight important distant tokens.\n",
        "* It improves contextual understanding.\n",
        "* This leads to better performance on complex language tasks.\n"
      ],
      "metadata": {
        "id": "BT6KmrUPbD4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14. What is the core principle behind the Transformer architecture**\n",
        "\n",
        "* The Transformer is based on the self-attention mechanism.\n",
        "* It removes recurrence and convolution completely.\n",
        "* All tokens in a sequence are processed simultaneously.\n",
        "* Self-attention captures contextual relationships effectively.\n",
        "* The architecture relies on stacked encoder and decoder layers.\n",
        "* Each layer contains attention and feed-forward networks.\n",
        "* Parallel processing improves efficiency and scalability.\n",
        "* This design achieves high performance in NLP tasks.\n"
      ],
      "metadata": {
        "id": "R8wQYoCGbNX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15. What is the role of the \"position encoding\" in a Transformer model**\n",
        "\n",
        "* Transformers do not have an inherent sense of word order.\n",
        "* Positional encoding provides information about token positions.\n",
        "* It is added to the input embeddings.\n",
        "* This helps the model understand sequence order.\n",
        "* Positional encodings can be sinusoidal or learned.\n",
        "* They allow attention to consider word positions.\n",
        "* Without them, word order would be lost.\n",
        "* Positional encoding improves sequence understanding.\n"
      ],
      "metadata": {
        "id": "cukoPrPdbVGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16. How do Transformers use multiple layers of attention**\n",
        "\n",
        "* Transformers stack multiple self-attention layers to capture hierarchical representations.\n",
        "* Each layer learns different levels of relationships between tokens.\n",
        "* Lower layers capture local or syntactic patterns, while higher layers capture semantic meaning.\n",
        "* Multiple layers allow the model to refine attention and context understanding progressively.\n",
        "* Each layer includes multi-head attention to focus on different parts of the sequence simultaneously.\n",
        "* Layer stacking improves the model's ability to represent complex dependencies.\n",
        "* This contributes to better performance on NLP tasks like translation, summarization, and question answering.\n"
      ],
      "metadata": {
        "id": "IqYqrmFSdKBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17. What does it mean when a model is described as “autoregressive” like GPT-2**\n",
        "\n",
        "* An autoregressive model predicts the next token in a sequence based on previous tokens.\n",
        "* It generates text one token at a time, using past outputs as context.\n",
        "* The model estimates the probability of the next word given the prior words.\n",
        "* GPT-2, GPT-3, and similar models are autoregressive in nature.\n",
        "* This allows coherent text generation but cannot see future tokens.\n",
        "* Autoregressive training encourages learning sequential dependencies in data.\n",
        "* It is different from bidirectional models like BERT that see both past and future tokens.\n"
      ],
      "metadata": {
        "id": "-DqU_Rfsb8ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18. How does BERT's bidirectional training improve its performance**\n",
        "\n",
        "* BERT uses bidirectional training to consider both left and right context of a token.\n",
        "* It masks certain words during training and predicts them using surrounding context.\n",
        "* This allows the model to understand full sentence meaning rather than just preceding tokens.\n",
        "* Bidirectional context improves tasks like question answering and sentiment analysis.\n",
        "* It captures deeper semantic relationships between words.\n",
        "* Unlike autoregressive models, BERT does not generate text sequentially but focuses on understanding.\n",
        "* This leads to better performance on NLP tasks requiring context comprehension.\n"
      ],
      "metadata": {
        "id": "zTOxAl8db_my"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19. What are the advantages of using the Transformer over RNN-based models in NLP**\n",
        "\n",
        "* Transformers process all tokens in a sequence simultaneously, enabling parallelization.\n",
        "* They handle long-range dependencies better than RNNs, which struggle with vanishing gradients.\n",
        "* Attention mechanisms allow the model to focus on relevant parts of the input dynamically.\n",
        "* Transformers are faster to train due to non-sequential computation.\n",
        "* They scale efficiently to very large datasets and models.\n",
        "* Unlike RNNs, Transformers do not rely on recurrence, avoiding long-term dependency bottlenecks.\n",
        "* They achieve state-of-the-art performance across many NLP tasks.\n"
      ],
      "metadata": {
        "id": "aoF1zQ6ecJru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20. What is the attention mechanism’s impact on the performance of models like BERT and GPT-2**\n",
        "\n",
        "* Attention allows models to weigh the importance of different tokens in a sequence.\n",
        "* It helps capture relationships between distant words effectively.\n",
        "* Improves context understanding for tasks like translation, summarization, and question answering.\n",
        "* Enables models to focus on relevant information rather than treating all tokens equally.\n",
        "* Self-attention in BERT and GPT-2 allows parallel processing of sequences.\n",
        "* Leads to better handling of long-range dependencies compared to RNNs.\n",
        "* Enhances overall performance and accuracy in NLP tasks.\n"
      ],
      "metadata": {
        "id": "XwV4g1eVcUEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "zDSpAx6ecWpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.How to implement a simple text classification model using LSTM in Keras**"
      ],
      "metadata": {
        "id": "AI46Hmp8eHps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Sample data\n",
        "texts = ['I love machine learning', 'Deep learning is great', 'I hate bugs', 'Debugging is fun']\n",
        "labels = np.array([1, 1, 0, 1])  # Convert labels to a numpy array\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=10)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(padded_sequences, labels, epochs=5)"
      ],
      "metadata": {
        "id": "JVu5cbY2bN1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.How to generate sequences of text using a Recurrent Neural Network (RNN)**"
      ],
      "metadata": {
        "id": "kbyFRtDzeqfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "text = \"hello world hello machine learning hello deep learning hello anmol hello pwskills\"\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "sequences = []\n",
        "\n",
        "# Create sequences\n",
        "for i in range(1, len(text.split())):\n",
        "    seq = text.split()[:i+1]\n",
        "    sequences.append([word_index[word] for word in seq])\n",
        "\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "# Split into X and y\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = np.eye(len(word_index)+1)[y]  # One-hot encode output\n",
        "\n",
        "# Build RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index)+1, output_dim=10, input_length=max_len-1))\n",
        "model.add(SimpleRNN(50))\n",
        "model.add(Dense(len(word_index)+1, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"hello\"\n",
        "for _ in range(10):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word = list(word_index.keys())[np.argmax(predicted)-1]\n",
        "    seed_text += \" \" + predicted_word\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "id": "BDfp72KkeMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.How to perform sentiment analysis using a simple CNN model**"
      ],
      "metadata": {
        "id": "2qphpQcGe4qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Sample data\n",
        "texts = ['I love this product', 'This is terrible', 'Amazing experience', 'Not good at all']\n",
        "labels = np.array([1, 0, 1, 0])  # 1 = positive, 0 = negative, convert to numpy array\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=10)\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=50, input_length=10))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(padded_sequences, labels, epochs=5)"
      ],
      "metadata": {
        "id": "_lCydkuAexMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4How to perform Named Entity Recognition (NER) using spaCy**"
      ],
      "metadata": {
        "id": "7GXQe7Cmfj2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy if not already installed\n",
        "# !pip install spacy\n",
        "\n",
        "# Import spaCy\n",
        "import spacy\n",
        "\n",
        "# Load pre-trained English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print named entities\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "_Y2uk2mZfcW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.How to implement a simple Seq2Seq model for machine translation using LSTM in Keras**"
      ],
      "metadata": {
        "id": "jMzXmReqf9T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Sample data (English to Spanish)\n",
        "input_texts = ['hello', 'how are you']\n",
        "target_texts = ['hola', 'como estas']\n",
        "\n",
        "# Tokenization\n",
        "input_characters = sorted(list(set(''.join(input_texts))))\n",
        "target_characters = sorted(list(set(''.join(target_texts))))\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t-1, target_token_index[char]] = 1.\n",
        "\n",
        "# Define encoder\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define decoder\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=2, epochs=100)\n"
      ],
      "metadata": {
        "id": "XBKWWplpf57u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.How to generate text using a pre-trained transformer model (GPT-2)**"
      ],
      "metadata": {
        "id": "Nr5zyaXtg5Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers if not already installed\n",
        "# !pip install transformers\n",
        "\n",
        "# Import libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Input prompt\n",
        "prompt = \"Once upon a time in a futuristic city\"\n",
        "\n",
        "# Encode input prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,       # total length including prompt\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    temperature=0.7,      # creativity factor\n",
        "    top_p=0.9,            # nucleus sampling\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "# Decode output to readable text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "Hzl7h8g6gLU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.How to apply data augmentation for text in NLP**"
      ],
      "metadata": {
        "id": "_k0-JbIghIST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary library\n",
        "!pip install nlpaug\n",
        "\n",
        "# Import libraries\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data for wordnet and pos tagging\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to fix the LookupError\n",
        "\n",
        "# Sample text\n",
        "text = \"I love machine learning and deep learning\"\n",
        "\n",
        "# --- 1. Synonym replacement ---\n",
        "syn_aug = naw.SynonymAug(aug_src='wordnet')  # replace words with synonyms\n",
        "aug_text1 = syn_aug.augment(text)\n",
        "print(\"Synonym Replacement:\", aug_text1)\n",
        "\n",
        "# --- 2. Random swap ---\n",
        "swap_aug = naw.RandomWordAug(action=\"swap\")  # swap positions of words\n",
        "aug_text3 = swap_aug.augment(text)\n",
        "print(\"Random Swap:\", aug_text3)\n",
        "\n",
        "# --- 3. Random deletion ---\n",
        "del_aug = naw.RandomWordAug(action=\"delete\")  # delete words randomly\n",
        "aug_text4 = del_aug.augment(text)\n",
        "print(\"Random Deletion:\", aug_text4)"
      ],
      "metadata": {
        "id": "aR_azC5ghDzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.How can you add an Attention Mechanism to a Seq2Seq model**"
      ],
      "metadata": {
        "id": "RhH09piniAqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TimeDistributed, Activation, Dot, RepeatVector\n",
        "\n",
        "# --- Sample dimensions ---\n",
        "encoder_vocab_size = 100\n",
        "decoder_vocab_size = 100\n",
        "embedding_dim = 64\n",
        "latent_dim = 50\n",
        "max_encoder_seq_length = 10\n",
        "max_decoder_seq_length = 10\n",
        "\n",
        "# --- Encoder ---\n",
        "encoder_inputs = Input(shape=(max_encoder_seq_length, encoder_vocab_size))\n",
        "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# --- Decoder ---\n",
        "decoder_inputs = Input(shape=(max_decoder_seq_length, decoder_vocab_size))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "# --- Attention mechanism ---\n",
        "# Compute attention scores\n",
        "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])\n",
        "attention = Activation('softmax')(attention)\n",
        "\n",
        "# Compute context vector\n",
        "context = Dot(axes=[2,1])([attention, encoder_outputs])\n",
        "\n",
        "# Concatenate context vector with decoder outputs\n",
        "decoder_combined_context = Concatenate(axis=-1)([context, decoder_outputs])\n",
        "\n",
        "# Output layer\n",
        "output_dense = TimeDistributed(Dense(decoder_vocab_size, activation='softmax'))\n",
        "decoder_outputs_final = output_dense(decoder_combined_context)\n",
        "\n",
        "# Define final model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs_final)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "oObF1zGfhUP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43FmyROdiKju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}