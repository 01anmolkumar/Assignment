{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwTbHnW8+fD2go1sUBzpBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01anmolkumar/Assignment/blob/main/Useful_NLP_Libraries_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **bold text** **Useful NLP Libraries and Networks**"
      ],
      "metadata": {
        "id": "F0g2xGWW2uxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is NLTK?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* NLTK (Natural Language Toolkit) is one of the most widely used libraries for Natural Language Processing (NLP) in Python.\n",
        "\n",
        "* It provides easy-to-use tools for text preprocessing such as tokenization, stemming, lemmatization, and stopword removal.\n",
        "\n",
        "* NLTK also includes datasets, corpora, and pre-trained models useful for NLP learning and experimentation.\n",
        "\n",
        "* It is mainly used for research, education, and building simple to moderate NLP applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "0iWW9fwW3Bfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is SpaCy and how does it differ from NLTK**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* SpaCy is an open-source, industrial-strength Natural Language Processing (NLP) library for Python.\n",
        "* It provides ready-to-use pipelines for tokenization, POS tagging, named entity recognition (NER), dependency parsing, lemmatization, and word vectors.\n",
        "* SpaCy is written for production — it is fast, memory-efficient, and has a clean, consistent API (many components implemented in Cython).\n",
        "\n",
        "* NLTK (Natural Language Toolkit) is a comprehensive library designed mostly for teaching and research.\n",
        "* NLTK provides many classic algorithms, utilities, datasets, and examples useful for learning NLP and experimenting with different techniques.\n",
        "* NLTK is more educational and experimental; it is modular but usually slower and less optimized for production workloads.\n",
        "\n",
        "* Key differences:\n",
        "  * **Speed & Efficiency:** SpaCy is significantly faster and more memory-efficient than NLTK.\n",
        "  * **Purpose:** SpaCy → production / real-world pipelines. NLTK → teaching, research, and prototyping.\n",
        "  * **API & Ease:** SpaCy gives a concise pipeline-based API; NLTK exposes many low-level tools and algorithms.\n",
        "  * **Pretrained Models & Vectors:** SpaCy ships with modern pretrained models and word vectors; NLTK relies on separate resources and focuses on algorithms and corpora.\n",
        "  * **Ecosystem:** SpaCy integrates well with modern ML stacks (e.g., Hugging Face, scikit-learn); NLTK is great for exploring classic NLP methods and datasets.\n",
        "\n",
        "* When to use:\n",
        "  * Use **SpaCy** when you need fast, reliable NLP in production or want a compact pipeline with pretrained models.\n",
        "  * Use **NLTK** when learning NLP concepts, experimenting with algorithms, or needing access to many corpora and educational examples.\n"
      ],
      "metadata": {
        "id": "420EhVAt3pkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is the purpose of TextBlob in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* TextBlob is a simple and beginner-friendly Natural Language Processing (NLP) library built on top of NLTK and Pattern.\n",
        "* It provides an easy and readable API to perform common NLP tasks with very little code.\n",
        "* TextBlob is mainly useful for quick text processing, learning NLP, and small-scale applications.\n",
        "\n",
        "* TextBlob supports many basic NLP features.\n",
        "* It includes sentiment analysis, tokenization, POS tagging, noun phrase extraction, and lemmatization.\n",
        "* It focuses more on simplicity rather than production-level speed or performance.\n",
        "\n",
        "* Key purposes:\n",
        "  * Sentiment Analysis → gives polarity and subjectivity scores.\n",
        "  * Text Processing → tokenization, lemmatization, chunking, POS tagging.\n",
        "  * Spelling Correction → automatic correction of misspelled words.\n",
        "  * Translation & Language Detection → uses Pattern API.\n",
        "  * Text Classification → supports simple Naive Bayes and classifiers.\n",
        "\n",
        "* When to use:\n",
        "  * Use TextBlob for quick, simple NLP tasks and easy sentiment analysis.\n",
        "  * Use it for school assignments, basic projects, or when learning NLP concepts.\n",
        "  * Ideal when you want clear syntax and do not need high-performance models.\n",
        "\n"
      ],
      "metadata": {
        "id": "VqbW6Mca7xiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is Stanford NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Stanford NLP (Stanford CoreNLP) is a powerful, open-source Natural Language Processing toolkit developed by Stanford University.\n",
        "* It provides a wide range of NLP tools such as tokenization, POS tagging, lemmatization, parsing, and coreference resolution.\n",
        "* It is written mainly in Java, but supports multiple languages through APIs (Python, JavaScript, etc.).\n",
        "\n",
        "* Stanford NLP is known for its high accuracy and linguistically rich models.\n",
        "* It includes advanced features like sentiment analysis, NER, dependency parsing, and relation extraction.\n",
        "* It uses statistical and deep learning-based models for robust language understanding.\n",
        "\n",
        "* Key features:\n",
        "  * High-quality NER, POS tagging, and dependency parsing.\n",
        "  * Coreference resolution to track references across sentences.\n",
        "  * Sentiment analysis for phrase and sentence-level opinions.\n",
        "  * Supports multiple languages through additional models.\n",
        "  * Integrates well with large-scale server-based NLP tasks.\n",
        "\n",
        "* When to use:\n",
        "  * Use Stanford NLP when you need high accuracy and linguistically deep analysis.\n",
        "  * Best for research, enterprise applications, or tasks needing advanced parsing.\n",
        "  * Suitable for large NLP pipelines and multilingual processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "9PN46VWP78RZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain what Recurrent Neural Networks (RNN) are**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data.\n",
        "* Unlike regular neural networks, RNNs have a memory that stores information from previous time steps.\n",
        "* This makes them suitable for tasks where order and context are important.\n",
        "\n",
        "* RNNs process input one element at a time while keeping a hidden state.\n",
        "* The hidden state helps the model remember previous information in the sequence.\n",
        "* RNNs share parameters across all time steps, making them efficient for sequence modeling.\n",
        "\n",
        "* Key features:\n",
        "  * Ability to handle sequential and time-dependent data.\n",
        "  * Uses a hidden state to store past information.\n",
        "  * Suitable for variable-length inputs (texts, speech, time-series).\n",
        "  * Can model context and relationships across steps.\n",
        "\n",
        "* Applications:\n",
        "  * NLP tasks such as text generation, language modeling, and sentiment analysis.\n",
        "  * Speech processing and audio recognition.\n",
        "  * Time-series prediction and forecasting.\n",
        "\n",
        "* Limitations:\n",
        "  * Struggle with long-term dependencies due to vanishing and exploding gradients.\n",
        "  * Training can be slower compared to modern architectures like LSTM or GRU.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RzFQ7NuN8DpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What is the main advantage of using LSTM over RNN**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* LSTM (Long Short-Term Memory) is an advanced version of RNN designed to solve the vanishing gradient problem.\n",
        "* It can remember long-term dependencies much better than a standard RNN.\n",
        "* LSTM uses special gates to control the flow of information.\n",
        "\n",
        "* Main advantage:\n",
        "  * LSTM can store and retrieve information over long sequences without forgetting.\n",
        "  * It prevents vanishing gradients during training, allowing deeper learning.\n",
        "  * It keeps important information for longer and removes irrelevant information.\n",
        "\n",
        "* Why LSTM is better than RNN:\n",
        "  * RNNs forget information quickly in long sequences.\n",
        "  * LSTMs use input, forget, and output gates to manage memory.\n",
        "  * LSTMs perform better on tasks requiring long-term contextual understanding.\n",
        "\n",
        "* Where this advantage matters:\n",
        "  * Text generation and sequence modeling.\n",
        "  * Machine translation.\n",
        "  * Speech recognition.\n",
        "  * Any task where long-term memory is needed.\n"
      ],
      "metadata": {
        "id": "kePSKYng8F-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What are Bi-directional LSTMs, and how do they differ from standard LSTMs**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* A Bi-directional LSTM (Bi-LSTM) is a type of LSTM that processes the input sequence in both directions: forward and backward.\n",
        "* It uses two LSTM layers — one reads the sequence from left to right, and the other from right to left.\n",
        "* The outputs of both directions are combined to capture full context.\n",
        "\n",
        "* Standard LSTMs read the input only in one direction (normally left to right).\n",
        "* They only use past context, not future context.\n",
        "* This limits the understanding when full context of the sequence matters.\n",
        "\n",
        "* Key differences:\n",
        "  * Bi-LSTM processes data in **two directions**, while standard LSTM processes in **one direction**.\n",
        "  * Bi-LSTMs capture **both past and future** information.\n",
        "  * Standard LSTMs capture only **past** information.\n",
        "  * Bi-LSTMs give better understanding of the entire sequence.\n",
        "\n",
        "* Applications:\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Part-of-Speech Tagging (POS)\n",
        "  * Sentiment Analysis\n",
        "  * Sequence labeling tasks where full context is important\n"
      ],
      "metadata": {
        "id": "NhKuEObf8c01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is the purpose of a Stacked LSTM**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* A Stacked LSTM is an architecture where multiple LSTM layers are placed on top of each other.\n",
        "* Each layer receives the output from the previous LSTM layer.\n",
        "* This allows the network to learn more complex patterns in the data.\n",
        "\n",
        "* Stacked LSTMs increase the model's depth.\n",
        "* Deeper networks can capture higher-level features and long-term dependencies.\n",
        "* They are more powerful than a single-layer LSTM.\n",
        "\n",
        "* Purpose:\n",
        "  * To improve the model’s ability to learn complex sequence relationships.\n",
        "  * To extract deeper and richer features from sequential data.\n",
        "  * To increase performance on advanced NLP and sequence tasks.\n",
        "\n",
        "* Benefits:\n",
        "  * Better feature representation at multiple levels.\n",
        "  * Improved accuracy on tasks like translation, text classification, and speech processing.\n",
        "  * Handles complicated patterns that a single LSTM cannot.\n",
        "\n",
        "* When to use:\n",
        "  * When the dataset is large and requires deep learning models.\n",
        "  * For complex NLP tasks such as machine translation, summarization, and sentiment analysis.\n",
        "  * When you need stronger modeling of long-term dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "MTv3GkKf9EdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. How does a GRU (Gated Recurrent Unit) differ from an LSTM**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* GRU (Gated Recurrent Unit) is a simplified version of LSTM designed to solve the vanishing gradient problem in RNNs.\n",
        "* It uses fewer gates and has a simpler structure compared to LSTM.\n",
        "* GRUs are faster to train and require less computational resources.\n",
        "\n",
        "* LSTMs use three gates: input gate, forget gate, and output gate.\n",
        "* GRUs use only two gates: update gate and reset gate.\n",
        "* GRUs combine the cell state and hidden state into a single state.\n",
        "\n",
        "* Key differences:\n",
        "  * GRU has **2 gates**, LSTM has **3 gates**.\n",
        "  * GRU has **no separate cell state**; LSTM has both cell state and hidden state.\n",
        "  * GRUs train **faster** because of fewer parameters.\n",
        "  * LSTMs can model more complex patterns due to their detailed gating system.\n",
        "  * GRUs are simpler and often perform similarly to LSTMs on many tasks.\n",
        "\n",
        "* When GRU is preferred:\n",
        "  * When computational power is limited.\n",
        "  * When training speed is important.\n",
        "  * When the dataset is smaller or simpler.\n",
        "\n",
        "* When LSTM is preferred:\n",
        "  * For more complex tasks requiring deeper memory.\n",
        "  * When long-term dependencies are very important.\n"
      ],
      "metadata": {
        "id": "pj-N-yIq9QHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What are the key features of NLTK's tokenization process**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* NLTK provides simple and flexible tokenization tools for breaking text into smaller units.\n",
        "* It supports both word-level and sentence-level tokenization.\n",
        "* Tokenizers in NLTK handle punctuation, spacing, and basic text structure efficiently.\n",
        "\n",
        "* NLTK includes multiple tokenizers such as word_tokenize, sent_tokenize, and regexp-based tokenizers.\n",
        "* It also has specialized tokenizers for tweets and informal text.\n",
        "* These tokenizers use pre-trained models based on the Punkt algorithm.\n",
        "\n",
        "* Key features:\n",
        "  * Word Tokenization → splits text into individual words and punctuation.\n",
        "  * Sentence Tokenization → identifies sentence boundaries accurately.\n",
        "  * Punkt Tokenizer → unsupervised model that learns sentence boundaries.\n",
        "  * RegexpTokenizer → allows custom token patterns using regular expressions.\n",
        "  * TweetTokenizer → handles hashtags, mentions, emojis, and slang.\n",
        "  * Language Support → tokenizers available for multiple languages.\n",
        "\n",
        "* Benefits:\n",
        "  * Easy to use with simple function calls.\n",
        "  * Flexible for both simple and advanced tokenization needs.\n",
        "  * Suitable for preprocessing steps in NLP tasks like classification and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "4yazGoeY9aZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. How do you perform named entity recognition (NER) using SpaCy**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* SpaCy provides a built-in Named Entity Recognition (NER) model in its language pipelines.\n",
        "* It can identify entities such as names, locations, organizations, dates, etc.\n",
        "* NER is performed automatically when a text is passed through the SpaCy model.\n",
        "\n",
        "* To perform NER, you load a SpaCy language model (like en_core_web_sm).\n",
        "* Then you process the text using nlp() to create a Doc object.\n",
        "* The recognized entities can be accessed through doc.ents.\n",
        "\n",
        "* Steps to perform NER in SpaCy:\n",
        "  * Import spaCy.\n",
        "  * Load a pre-trained model (en_core_web_sm).\n",
        "  * Pass the text to the nlp model.\n",
        "  * Loop through doc.ents to extract entity text and labels.\n",
        "\n",
        "* Example:\n",
        "  * nlp = spacy.load(\"en_core_web_sm\")\n",
        "  * doc = nlp(\"Apple is planning to open a new office in India.\")\n",
        "  * for ent in doc.ents:\n",
        "        print(ent.text, ent.label_)\n",
        "\n",
        "* What it detects:\n",
        "  * PERSON, ORG, GPE, DATE, MONEY, TIME, PRODUCT, etc.\n",
        "\n",
        "* Benefits:\n",
        "  * Fast and accurate entity recognition.\n",
        "  * Easy to implement with minimal code.\n",
        "  * Useful for tasks like information extraction, summarization, and text analysis.\n"
      ],
      "metadata": {
        "id": "qvoo6qbg9dse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. What is Word2Vec and how does it represent words**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Word2Vec is a popular word embedding technique developed by Google.\n",
        "* It converts words into continuous numerical vectors.\n",
        "* These vectors capture semantic meaning and relationships between words.\n",
        "\n",
        "* Word2Vec represents words using dense, low-dimensional vectors.\n",
        "* Words with similar meanings have similar vector representations.\n",
        "* It learns these representations from large text corpora using neural networks.\n",
        "\n",
        "* Key concepts:\n",
        "  * Distributed Representation → each word is represented by a dense vector.\n",
        "  * Semantic Similarity → similar words are located close in vector space.\n",
        "  * Context-Based Learning → words are learned based on their surrounding words.\n",
        "\n",
        "* Word2Vec uses two main models:\n",
        "  * CBOW (Continuous Bag of Words) → predicts a word from its context.\n",
        "  * Skip-Gram → predicts the context words from a target word.\n",
        "\n",
        "* Advantages:\n",
        "  * Captures semantic relationships like king - man + woman = queen.\n",
        "  * More meaningful than one-hot vectors or Bag-of-Words.\n",
        "  * Efficient to train on large datasets.\n",
        "\n",
        "* Uses:\n",
        "  * Text classification, sentiment analysis, translation, recommendation systems.\n",
        "  * Understanding similarity between words.\n"
      ],
      "metadata": {
        "id": "YdsaXEcH9wGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Explain the difference between Bag of Words (BoW) and Word2VecK**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Bag of Words (BoW) is a simple text representation technique that converts text into a sparse vector of word counts or frequencies.**\n",
        "* **It ignores grammar, word order, and context.**\n",
        "* **Each word is treated independently (one-hot-like representation).**\n",
        "* **High-dimensional if vocabulary is large.**\n",
        "* **Simple and easy to implement.**\n",
        "\n",
        "* **Word2Vec is a word embedding technique that represents words as dense, low-dimensional vectors.**\n",
        "* **It captures semantic meaning and relationships between words.**\n",
        "* **Considers context using surrounding words (CBOW or Skip-Gram models).**\n",
        "* **More memory-efficient than BoW.**\n",
        "* **Allows computation of word similarity and analogies.**\n",
        "\n",
        "* **Key Concepts / Differences:**\n",
        "  * **Representation → BoW uses sparse vectors, Word2Vec uses dense embeddings.**\n",
        "  * **Context → BoW ignores context, Word2Vec captures context.**\n",
        "  * **Semantic Meaning → BoW does not capture meaning, Word2Vec captures semantic relationships.**\n",
        "  * **Dimensionality → BoW is high-dimensional, Word2Vec is low-dimensional.**\n",
        "  * **Use Cases → BoW is used for simple text classification, Word2Vec is used for semantic tasks and similarity analysis.**\n"
      ],
      "metadata": {
        "id": "A2fOhISk_dc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. How does TextBlob handle sentiment analysis**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **TextBlob is a Python library for processing textual data, including sentiment analysis.**\n",
        "* **It analyzes the polarity and subjectivity of text.**\n",
        "* **Polarity ranges from -1 to 1, where -1 indicates negative sentiment, 0 is neutral, and 1 is positive.**\n",
        "* **Subjectivity ranges from 0 to 1, where 0 is very objective and 1 is very subjective.**\n",
        "* **TextBlob uses a pre-trained lexicon-based approach, assigning sentiment scores to words and combining them to compute overall sentiment.**\n",
        "* **It can handle sentences, phrases, or entire documents.**\n",
        "* **TextBlob is simple to use and integrates easily with Python projects for tasks like opinion mining and social media analysis.**\n"
      ],
      "metadata": {
        "id": "fwpSFx5E_t91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. How would you implement text preprocessing using NLTK**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **NLTK (Natural Language Toolkit) is a Python library used for text preprocessing and NLP tasks.**\n",
        "* **Common text preprocessing steps using NLTK include:**\n",
        "  * **Tokenization → splitting text into words or sentences using `word_tokenize()` or `sent_tokenize()`.**\n",
        "  * **Lowercasing → converting all text to lowercase to ensure uniformity.**\n",
        "  * **Stopword Removal → removing common words like \"is\", \"the\", \"and\" using `stopwords.words()`.**\n",
        "  * **Stemming → reducing words to their root form using `PorterStemmer` or `LancasterStemmer`.**\n",
        "  * **Lemmatization → reducing words to their base dictionary form using `WordNetLemmatizer`.**\n",
        "  * **Removing punctuation and special characters → cleaning text for better analysis.**\n",
        "* **Example Implementation:**\n",
        "* **This process prepares text for tasks like classification, sentiment analysis, and other NLP applications.**\n"
      ],
      "metadata": {
        "id": "uOW0WiOA_9Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. How do you train a custom NER model using SpaCy**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **SpaCy is a Python library for advanced NLP tasks, including Named Entity Recognition (NER).**\n",
        "* **To train a custom NER model, follow these steps:**\n",
        "\n",
        "  * **Step 1: Prepare Training Data → Create a list of tuples with text and entities in the format:**\n",
        "    ```\n",
        "    TRAIN_DATA = [\n",
        "        (\"Apple is looking at buying U.K. startup for $1 billion\", {\"entities\": [(0, 5, \"ORG\"), (27, 30, \"GPE\"), (44, 54, \"MONEY\")]}),\n",
        "        ...\n",
        "    ]\n",
        "    ```\n",
        "\n",
        "  * **Step 2: Load or Create a SpaCy Model → Either start with a blank model or an existing one:**\n",
        "    ```\n",
        "    import spacy\n",
        "    nlp = spacy.blank(\"en\")  # blank English model\n",
        "    ner = nlp.add_pipe(\"ner\")\n",
        "    ```\n",
        "\n",
        "  * **Step 3: Add Labels → Add entity labels to the NER pipeline:**\n",
        "    ```\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "    ```\n",
        "\n",
        "  * **Step 4: Train the Model → Use the `nlp.update()` function in a loop:**\n",
        "    ```\n",
        "    import random\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(20):\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        for text, annotations in TRAIN_DATA:\n",
        "            nlp.update([text], [annotations], sgd=optimizer)\n",
        "    ```\n",
        "\n",
        "  * **Step 5: Test the Model → Check predictions on new text:**\n",
        "    ```\n",
        "    doc = nlp(\"Apple plans to acquire a UK startup\")\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.label_)\n",
        "    ```\n",
        "\n",
        "* **SpaCy allows fine-tuning pre-trained models or creating models from scratch for custom entity recognition tasks.**\n",
        "* **Custom NER models are useful in domains like finance, healthcare, or legal text where standard models may not perform well.**\n"
      ],
      "metadata": {
        "id": "mA_5Sz4vAUh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. What is the role of the attention mechanism in LSTMs and GRUs**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **The attention mechanism is used in LSTMs and GRUs to improve the handling of long sequences.**\n",
        "* **It allows the model to focus on the most relevant parts of the input sequence when making predictions.**\n",
        "* **Instead of relying only on the last hidden state, attention assigns weights to all hidden states, highlighting important information.**\n",
        "* **Key Benefits:**\n",
        "  * **Helps capture long-range dependencies more effectively.**\n",
        "  * **Improves performance in tasks like machine translation, text summarization, and question answering.**\n",
        "  * **Makes the model interpretable by showing which parts of the input influenced the output.**\n",
        "* **Mechanism Overview:**\n",
        "  * **Compute a score for each hidden state based on its relevance to the current output.**\n",
        "  * **Normalize scores using softmax to get attention weights.**\n",
        "  * **Multiply hidden states by attention weights and sum them to get a context vector.**\n",
        "  * **Feed the context vector to the decoder or output layer for prediction.**\n",
        "* **Overall, attention helps LSTMs and GRUs selectively focus on important input features, improving sequence modeling.**\n"
      ],
      "metadata": {
        "id": "6vQ9JSNEAnRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. What is the difference between tokenization and lemmatization in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Tokenization is the process of breaking text into smaller units called tokens, such as words, phrases, or sentences.**  \n",
        "* **It helps in analyzing and processing text by creating manageable pieces for NLP tasks.**  \n",
        "* **Example → \"NLTK is useful\" → [\"NLTK\", \"is\", \"useful\"]**  \n",
        "\n",
        "* **Lemmatization is the process of reducing a word to its base or dictionary form called lemma.**  \n",
        "* **It considers the context and part of speech to convert words to their root form.**  \n",
        "* **Example → \"running\", \"ran\", \"runs\" → \"run\"**  \n",
        "\n",
        "* **Key Differences:**  \n",
        "  * **Purpose → Tokenization splits text into tokens, lemmatization normalizes words to their base form.**  \n",
        "  * **Output → Tokenization outputs multiple pieces of text, lemmatization outputs a single normalized word.**  \n",
        "  * **Dependency → Lemmatization usually requires tokenization first.**  \n",
        "  * **Use Cases → Tokenization is used in almost all NLP tasks, lemmatization is used in text normalization, search, and preprocessing.**  \n"
      ],
      "metadata": {
        "id": "v9W9M31sA-jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. How do you perform text normalization in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Text normalization is the process of converting text into a standard, uniform format for easier analysis.**  \n",
        "* **Common steps in text normalization include:**  \n",
        "  * **Lowercasing → converting all text to lowercase to avoid case sensitivity.**  \n",
        "  * **Removing punctuation and special characters → cleaning text for better processing.**  \n",
        "  * **Removing numbers → if numeric data is not needed for analysis.**  \n",
        "  * **Expanding contractions → converting \"don't\" → \"do not\".**  \n",
        "  * **Tokenization → splitting text into words or sentences.**  \n",
        "  * **Stopword removal → removing common words like \"is\", \"the\", \"and\".**  \n",
        "  * **Stemming → reducing words to their root form (e.g., \"running\" → \"run\").**  \n",
        "  * **Lemmatization → reducing words to their base dictionary form (e.g., \"better\" → \"good\").**  \n",
        "\n",
        "* **Text normalization is essential for tasks like text classification, sentiment analysis, and machine learning models to perform accurately.**  \n"
      ],
      "metadata": {
        "id": "acoflqdcBEk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. What are the main steps involved in text preprocessing for NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Text preprocessing is the process of cleaning and preparing text for NLP tasks.**  \n",
        "* **Main steps include:**  \n",
        "  * **Lowercasing → converting all text to lowercase for uniformity.**  \n",
        "  * **Removing punctuation and special characters → cleaning text.**  \n",
        "  * **Removing numbers → if numeric data is not needed.**  \n",
        "  * **Expanding contractions → e.g., \"can't\" → \"cannot\".**  \n",
        "  * **Tokenization → splitting text into words or sentences.**  \n",
        "  * **Stopword removal → removing common words like \"is\", \"the\", \"and\".**  \n",
        "  * **Stemming → reducing words to their root form (e.g., \"running\" → \"run\").**  \n",
        "  * **Lemmatization → reducing words to their base dictionary form (e.g., \"better\" → \"good\").**  \n",
        "  * **POS tagging → labeling words with their part of speech for better understanding.**  \n",
        "  * **Named Entity Recognition (NER) → identifying proper nouns like names, locations, dates.**  \n",
        "\n",
        "* **Text preprocessing ensures that the text is clean, consistent, and ready for tasks like classification, sentiment analysis, and language modeling.**  \n"
      ],
      "metadata": {
        "id": "5cqUrM7tBZl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. What are co-occurrence vectors in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Co-occurrence vectors represent words based on how frequently they appear together with other words in a given context.**  \n",
        "* **They are a type of word representation that captures the relationships between words in a corpus.**  \n",
        "* **Typically constructed using a co-occurrence matrix, where rows and columns represent words, and each cell contains the count of how often the words appear together within a context window.**  \n",
        "* **These vectors help capture semantic similarity → words appearing in similar contexts have similar vectors.**  \n",
        "* **Used in techniques like GloVe (Global Vectors) to learn word embeddings from co-occurrence statistics.**  \n",
        "* **Advantages:**  \n",
        "  * **Captures statistical relationships between words.**  \n",
        "  * **Useful for building semantic representations and understanding word similarity.**  \n"
      ],
      "metadata": {
        "id": "AAin3YFQCWdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. How is Word2Vec used to find the relationship between words**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Word2Vec is a word embedding technique that represents words as dense, low-dimensional vectors.**  \n",
        "* **It captures semantic meaning by placing similar words close together in vector space.**  \n",
        "* **Two main models are used in Word2Vec:**  \n",
        "  * **CBOW (Continuous Bag of Words) → predicts a target word based on its surrounding context words.**  \n",
        "  * **Skip-Gram → predicts surrounding context words given a target word.**  \n",
        "* **Word relationships can be computed using vector arithmetic.**  \n",
        "  * **Example → vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")**  \n",
        "* **Cosine similarity is often used to find how similar or related two words are based on their vectors.**  \n",
        "* **Word2Vec is widely used in NLP tasks like analogy solving, semantic similarity, and recommendation systems.**  \n"
      ],
      "metadata": {
        "id": "ayo31FdDClAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. How does a Bi-LSTM improve NLP tasks compared to a regular LSTM**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Bi-LSTM (Bidirectional LSTM) is an extension of LSTM that processes the input sequence in both forward and backward directions.**  \n",
        "* **A regular LSTM only processes the sequence in one direction (usually forward).**  \n",
        "* **By reading the sequence in both directions, Bi-LSTM captures past (previous words) and future (next words) context simultaneously.**  \n",
        "* **Key Benefits:**  \n",
        "  * **Improved understanding of context → helps in tasks where both previous and next words are important.**  \n",
        "  * **Better performance in NLP tasks like named entity recognition (NER), part-of-speech tagging, and machine translation.**  \n",
        "  * **Helps in disambiguating words based on full sentence context.**  \n",
        "* **Overall, Bi-LSTM provides richer representations and improves sequence modeling compared to a unidirectional LSTM.**  \n"
      ],
      "metadata": {
        "id": "IaTH9c4tDKnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24. What is the difference between a GRU and an LSTM in terms of gate structures**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are types of recurrent neural networks designed to handle long-term dependencies.**  \n",
        "* **LSTM has three gates:**  \n",
        "  * **Input Gate → controls how much new information flows into the cell state.**  \n",
        "  * **Forget Gate → decides what information to discard from the cell state.**  \n",
        "  * **Output Gate → controls how much of the cell state is used to produce the output.**  \n",
        "* **GRU has two gates:**  \n",
        "  * **Update Gate → combines the functions of LSTM's input and forget gates to control how much information is updated.**  \n",
        "  * **Reset Gate → decides how much past information to forget when computing the new memory content.**  \n",
        "* **Key Differences:**  \n",
        "  * **GRUs are simpler and have fewer parameters than LSTMs.**  \n",
        "  * **GRUs often train faster and perform similarly on many tasks.**  \n",
        "  * **LSTMs may perform better on very long sequences due to separate memory cell and output gate.**  \n"
      ],
      "metadata": {
        "id": "xxBxOvSZDYK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25. How does Stanford NLP’s dependency parsing work**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Dependency parsing analyzes the grammatical structure of a sentence by establishing relationships between “head” words and words that modify them.**  \n",
        "* **Stanford NLP uses a probabilistic or neural-based dependency parser to identify these relationships.**  \n",
        "* **Key Steps:**  \n",
        "  * **Tokenization → splits the sentence into words or tokens.**  \n",
        "  * **Part-of-Speech (POS) Tagging → assigns a grammatical category (noun, verb, adjective, etc.) to each token.**  \n",
        "  * **Parsing → builds a dependency tree showing how words are connected.**  \n",
        "* **Dependency Relations:**  \n",
        "  * **Each word is linked to its head word with a labeled edge describing the grammatical relation (e.g., subject, object, modifier).**  \n",
        "* **Example:**  \n",
        "  * **Sentence → “The cat sat on the mat.”**  \n",
        "  * **Dependency → “sat” is the root, “cat” is subject, “mat” is object of preposition “on”.**  \n",
        "* **Applications:**  \n",
        "  * **Information extraction, question answering, sentiment analysis, and relation extraction.**  \n"
      ],
      "metadata": {
        "id": "n_QHmNFxDrOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26. How does tokenization affect downstream NLP tasks**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Tokenization is the process of breaking text into smaller units called tokens (words, subwords, or sentences).**  \n",
        "* **It is a critical first step in most NLP pipelines and directly affects model performance.**  \n",
        "* **Impacts on downstream tasks:**  \n",
        "  * **Text Representation → tokenization determines the input units for models like BoW, TF-IDF, or embeddings.**  \n",
        "  * **Vocabulary Size → incorrect tokenization can create a very large or sparse vocabulary.**  \n",
        "  * **Context Understanding → suboptimal tokenization may split meaningful words incorrectly, losing context.**  \n",
        "  * **Sequence Modeling → affects LSTM, GRU, Transformer models since sequence length and token order matter.**  \n",
        "  * **Performance of NLP Tasks → tasks like sentiment analysis, machine translation, and NER rely on accurate tokens for better results.**  \n",
        "* **Choosing the right tokenization method (word-level, subword, or sentence-level) is essential for optimal performance.**  \n"
      ],
      "metadata": {
        "id": "2j3Nzxk4D8sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**27. What are some common applications of NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Text Classification → categorizing text into predefined categories (e.g., spam detection, topic classification).**  \n",
        "* **Sentiment Analysis → determining the sentiment of text as positive, negative, or neutral.**  \n",
        "* **Machine Translation → automatically translating text from one language to another.**  \n",
        "* **Named Entity Recognition (NER) → identifying entities like names, locations, dates, and organizations in text.**  \n",
        "* **Question Answering → extracting answers to questions from a given text or database.**  \n",
        "* **Chatbots and Virtual Assistants → understanding and responding to user queries in natural language.**  \n",
        "* **Text Summarization → generating concise summaries from longer documents.**  \n",
        "* **Speech Recognition → converting spoken language into written text for further NLP processing.**  \n",
        "* **Information Retrieval → improving search engines by understanding query intent and document relevance.**  \n",
        "* **Recommendation Systems → analyzing text reviews or content to provide personalized suggestions.**  \n"
      ],
      "metadata": {
        "id": "aejl0alNEc5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**28. What are stopwords and why are they removed in NLP**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Stopwords are common words in a language that carry little meaningful information, such as \"is\", \"the\", \"and\", \"in\".**  \n",
        "* **They are usually removed during text preprocessing to reduce noise and improve efficiency.**  \n",
        "* **Reasons for removing stopwords:**  \n",
        "  * **Reduce dimensionality → decreases the size of feature vectors in models like BoW or TF-IDF.**  \n",
        "  * **Focus on meaningful words → helps models capture important information for tasks like classification or sentiment analysis.**  \n",
        "  * **Improve computational efficiency → fewer words to process speeds up training and inference.**  \n",
        "* **Stopword removal is optional depending on the NLP task; some tasks like language modeling may retain them.**  \n"
      ],
      "metadata": {
        "id": "wUS7tgbyExA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29. How can you implement word embeddings using Word2Vec in Python**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Word2Vec is a technique to convert words into dense, low-dimensional vectors capturing semantic meaning.**  \n",
        "* **Implementation in Python is commonly done using the `gensim` library.**  \n",
        "* **Steps to implement Word2Vec:**  \n",
        "  * **Step 1: Install and import gensim:**  \n",
        "    ```\n",
        "    !pip install gensim\n",
        "    from gensim.models import Word2Vec\n",
        "    ```  \n",
        "  * **Step 2: Prepare training data → tokenize sentences into lists of words:**  \n",
        "    ```\n",
        "    sentences = [[\"I\", \"love\", \"NLP\"], [\"Word2Vec\", \"creates\", \"word\", \"embeddings\"]]\n",
        "    ```  \n",
        "  * **Step 3: Train the Word2Vec model:**  \n",
        "    ```\n",
        "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    ```  \n",
        "  * **Step 4: Access word vectors:**  \n",
        "    ```\n",
        "    vector = model.wv[\"NLP\"]\n",
        "    ```  \n",
        "  * **Step 5: Find similar words:**  \n",
        "    ```\n",
        "    similar_words = model.wv.most_similar(\"NLP\")\n",
        "    print(similar_words)\n",
        "    ```  \n",
        "* **Word2Vec can use CBOW or Skip-Gram models depending on the `sg` parameter (`sg=0` for CBOW, `sg=1` for Skip-Gram).**  \n",
        "* **These embeddings can be used in NLP tasks like classification, clustering, or semantic similarity.**  \n"
      ],
      "metadata": {
        "id": "cA_15JgCFxj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**30. How does SpaCy handle lemmatization**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **SpaCy performs lemmatization by reducing words to their base or dictionary form, called a lemma.**  \n",
        "* **It uses a combination of rule-based and dictionary-based approaches to determine the correct lemma.**  \n",
        "* **Steps in SpaCy lemmatization:**  \n",
        "  * **Load a SpaCy language model:**  \n",
        "    ```\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    ```  \n",
        "  * **Process text:**  \n",
        "    ```\n",
        "    doc = nlp(\"running runners ran\")\n",
        "    ```  \n",
        "  * **Access lemmas:**  \n",
        "    ```\n",
        "    for token in doc:\n",
        "        print(token.text, token.lemma_)\n",
        "    ```  \n",
        "* **SpaCy considers part-of-speech (POS) tags to accurately lemmatize words.**  \n",
        "* **Lemmatization is useful in text preprocessing for tasks like search, text normalization, and NLP modeling.**  \n"
      ],
      "metadata": {
        "id": "65Aj38mRF_4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**31. What is the significance of RNNs in NLP tasks**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **RNNs (Recurrent Neural Networks) are neural networks designed to process sequential data like text.**  \n",
        "* **They maintain a hidden state that captures information from previous time steps, allowing them to model sequences.**  \n",
        "* **Significance in NLP:**  \n",
        "  * **Sequence Modeling → can handle variable-length text sequences such as sentences or documents.**  \n",
        "  * **Captures Context → remembers information from previous words to influence predictions.**  \n",
        "  * **Applications → used in language modeling, text generation, sentiment analysis, and machine translation.**  \n",
        "  * **Handles Temporal Dependencies → important for tasks where word order and context matter.**  \n",
        "* **Limitations → traditional RNNs struggle with long-term dependencies due to vanishing gradients, often addressed by LSTMs or GRUs.**  \n"
      ],
      "metadata": {
        "id": "NG9rUpBXGKXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**32. How does word embedding improve the performance of NLP models**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **Word embeddings are dense, low-dimensional vector representations of words that capture semantic meaning.**  \n",
        "* **They improve NLP model performance by providing meaningful numerical representations of words.**  \n",
        "* **Key Benefits:**  \n",
        "  * **Captures Semantic Relationships → similar words have similar vectors, enabling models to understand context.**  \n",
        "  * **Reduces Dimensionality → more efficient than sparse representations like one-hot vectors or BoW.**  \n",
        "  * **Improves Generalization → embeddings allow models to learn relationships between unseen words.**  \n",
        "  * **Enables Analogies → vector arithmetic can capture word relationships (e.g., king - man + woman ≈ queen).**  \n",
        "* **Word embeddings can be pre-trained (e.g., Word2Vec, GloVe) or trained on task-specific data to enhance performance in tasks like text classification, sentiment analysis, and translation.**  \n"
      ],
      "metadata": {
        "id": "JVN2WHVNGLw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**33. How does a Stacked LSTM differ from a single LSTM**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **A single LSTM consists of one layer of LSTM units that processes sequential data.**  \n",
        "* **A Stacked LSTM consists of multiple LSTM layers stacked on top of each other, where the output of one layer is the input to the next.**  \n",
        "* **Key Differences:**  \n",
        "  * **Representation Power → Stacked LSTMs can capture more complex patterns and hierarchical features in sequences.**  \n",
        "  * **Depth → single LSTM has one hidden layer, stacked LSTM has multiple hidden layers.**  \n",
        "  * **Performance → stacked LSTMs often perform better on complex NLP tasks like machine translation and text generation.**  \n",
        "  * **Computational Cost → stacked LSTMs are more computationally expensive and require more training data.**  \n",
        "* **Overall, stacked LSTMs provide deeper sequence modeling capability compared to a single LSTM.**  \n"
      ],
      "metadata": {
        "id": "P3dXVJuXGiGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**34. What are the key differences between RNN, LSTM, and GRU**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **RNN (Recurrent Neural Network):**  \n",
        "  * **Processes sequential data and maintains a hidden state to capture information from previous time steps.**  \n",
        "  * **Struggles with long-term dependencies due to vanishing or exploding gradients.**  \n",
        "  * **Simpler architecture with fewer parameters.**  \n",
        "\n",
        "* **LSTM (Long Short-Term Memory):**  \n",
        "  * **Addresses RNN limitations with memory cells and three gates: input, forget, and output.**  \n",
        "  * **Can capture long-term dependencies effectively.**  \n",
        "  * **More complex and computationally heavier than RNN.**  \n",
        "\n",
        "* **GRU (Gated Recurrent Unit):**  \n",
        "  * **Simplified version of LSTM with two gates: update and reset.**  \n",
        "  * **Combines input and forget gates of LSTM into a single update gate.**  \n",
        "  * **Fewer parameters, faster training, often performs similarly to LSTM.**  \n",
        "\n",
        "* **Key Differences Summary:**  \n",
        "  | Feature | RNN | LSTM | GRU |  \n",
        "  |---------|-----|------|-----|  \n",
        "  | Gates | None | Input, Forget, Output | Update, Reset |  \n",
        "  | Memory | Limited | Long-term memory | Long-term memory |  \n",
        "  | Complexity | Low | High | Medium |  \n",
        "  | Training Speed | Fast | Slower | Faster than LSTM |  \n"
      ],
      "metadata": {
        "id": "gD6ZcanIGu4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**35. Why is the attention mechanism important in sequence-to-sequence models**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* **The attention mechanism allows sequence-to-sequence models to focus on the most relevant parts of the input sequence when generating each output token.**  \n",
        "* **It overcomes the limitation of encoding the entire input sequence into a single fixed-length vector.**  \n",
        "* **Key Benefits:**  \n",
        "  * **Improves handling of long sequences → model can refer to any part of the input, not just the last hidden state.**  \n",
        "  * **Enhances translation, summarization, and other seq2seq tasks by focusing on relevant words.**  \n",
        "  * **Provides interpretability → attention weights indicate which input tokens influence the output.**  \n",
        "* **Mechanism Overview:**  \n",
        "  * **Compute alignment scores between the current decoder state and all encoder states.**  \n",
        "  * **Normalize scores with softmax to get attention weights.**  \n",
        "  * **Compute a context vector as the weighted sum of encoder states.**  \n",
        "  * **Use context vector to generate the next output token.**  \n",
        "* **Overall, attention improves accuracy, context understanding, and interpretability in seq2seq models.**  \n"
      ],
      "metadata": {
        "id": "jPTUsz-rHOck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "ZIPir9JbHZBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.How do you perform word tokenization using NLTK and plot a word frequency distribution\u0017**"
      ],
      "metadata": {
        "id": "sf7oOSitID6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "# Download punkt tokenizer if not already done\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Add this line to download the specific missing resource\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is fun. NLP allows computers to understand human language.\"\n",
        "\n",
        "# Tokenize words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Clean tokens: lowercase & remove punctuation\n",
        "words = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "# Frequency distribution\n",
        "fdist = FreqDist(words)\n",
        "print(\"Most common words:\", fdist.most_common(10))\n",
        "\n",
        "# Plot top 10 word frequencies\n",
        "fdist.plot(10, title=\"Top 10 Word Frequencies\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z3AnjhK8BD2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.How do you use SpaCy for dependency parsing of a sentence\u0017**"
      ],
      "metadata": {
        "id": "__V8HIZPIqjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Parse the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print token, POS, dependency, and head for each word\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}, Head: {token.head.text}\")\n",
        "\n",
        "# Visualize dependency tree (works in Jupyter)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True, options={\"distance\": 100})\n"
      ],
      "metadata": {
        "id": "hl30oHe1Ia7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.How do you use TextBlob for performing text classification based on polarity\u0017**"
      ],
      "metadata": {
        "id": "RDhYDRgZI00U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample texts\n",
        "texts = [\n",
        "    \"I love this movie. It was amazing!\",\n",
        "    \"The food was terrible and the service was slow.\",\n",
        "    \"It was an average experience, nothing special.\"\n",
        "]\n",
        "\n",
        "# Analyze each text\n",
        "for text in texts:\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "\n",
        "    # Classify sentiment based on polarity\n",
        "    if polarity > 0:\n",
        "        sentiment = \"Positive\"\n",
        "    elif polarity < 0:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Polarity: {polarity}, Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "id": "GKqTvBSiIpr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4How do you extract named entities from a text using SpaCy\u0017**"
      ],
      "metadata": {
        "id": "SdR__DVAKUOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Parse the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print named entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "\n",
        "# Visualize named entities (works in Jupyter)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ],
      "metadata": {
        "id": "h4vMunfdJ79b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.How can you calculate TF-IDF scores for a given text using Scikit-learn**"
      ],
      "metadata": {
        "id": "MHE0Jor8L1ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample text(s)\n",
        "texts = [\"I love Natural Language Processing and machine learning.\"]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert to DataFrame to see TF-IDF scores\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "LInoE9FUL48l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.How do you create a custom text classifier using NLTK's Naive Bayes classifier\u0017**"
      ],
      "metadata": {
        "id": "heltCx2KMeTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample training data: (text, label)\n",
        "train_data = [\n",
        "    (\"I love this movie, it's amazing!\", \"Positive\"),\n",
        "    (\"This film was terrible and boring.\", \"Negative\"),\n",
        "    (\"What a fantastic performance!\", \"Positive\"),\n",
        "    (\"I did not like the plot of the movie.\", \"Negative\")\n",
        "]\n",
        "\n",
        "# Preprocessing function: tokenize, lowercase, remove punctuation and stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return {word: True for word in tokens if word not in stop_words and word not in string.punctuation}\n",
        "\n",
        "# Prepare training features\n",
        "train_features = [(preprocess(text), label) for (text, label) in train_data]\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "classifier = NaiveBayesClassifier.train(train_features)\n",
        "\n",
        "# Test the classifier\n",
        "test_text = \"The movie was awesome and fun!\"\n",
        "test_features = preprocess(test_text)\n",
        "print(\"Text:\", test_text)\n",
        "print(\"Predicted Sentiment:\", classifier.classify(test_features))\n",
        "\n",
        "# Show most informative features\n",
        "classifier.show_most_informative_features(5)\n"
      ],
      "metadata": {
        "id": "WPPgKDPTMUpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you use a pre-trained model from Hugging Face for text classification\u0017**"
      ],
      "metadata": {
        "id": "2K0qtQeVMzOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the sentiment-analysis pipeline (uses a pre-trained model)\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Sample text\n",
        "text = \"I really love this movie! It was fantastic and exciting.\"\n",
        "\n",
        "# Perform classification\n",
        "result = classifier(text)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "YYLKXyd0Motp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.How do you perform text summarization using Hugging Face transformers\u0017**"
      ],
      "metadata": {
        "id": "jo2fKQT3NDA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline (uses a pre-trained model)\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.\n",
        "Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions\n",
        "that maximize its chance of achieving its goals. Some popular applications of AI include machine learning, natural language processing,\n",
        "computer vision, and robotics.\n",
        "\"\"\"\n",
        "\n",
        "# Perform summarization\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "\n",
        "# Print the summarized text\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "NKVJ4vnNM3Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.How can you create a simple RNN for text classification using Keras\u0017**"
      ],
      "metadata": {
        "id": "fQEmXZySN1B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Sample text data and labels\n",
        "texts = [\"I love this movie\", \"This film was terrible\", \"Amazing performance\", \"I did not like the plot\"]\n",
        "labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Build RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=16, input_length=5),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, labels, epochs=10, verbose=1)\n",
        "\n",
        "# Test on a new sentence\n",
        "test_text = [\"I really enjoyed the movie\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=5)\n",
        "prediction = model.predict(test_pad)\n",
        "print(\"Predicted Probability of Positive Sentiment:\", prediction[0][0])\n"
      ],
      "metadata": {
        "id": "T4rIl7NYNi5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.How do you train a Bidirectional LSTM for text classification\u0017**"
      ],
      "metadata": {
        "id": "oLpecAKNQs_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "# Sample text data and labels\n",
        "texts = [\"I love this movie\", \"This film was terrible\", \"Amazing performance\", \"I did not like the plot\"]\n",
        "labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Build Bidirectional LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=16, input_length=5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, labels, epochs=10, verbose=1)\n",
        "\n",
        "# Test on a new sentence\n",
        "test_text = [\"I really enjoyed the movie\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=5)\n",
        "prediction = model.predict(test_pad)\n",
        "print(\"Predicted Probability of Positive Sentiment:\", prediction[0][0])\n"
      ],
      "metadata": {
        "id": "kyb20S6fQPiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.How do you implement GRU (Gated Recurrent Unit) for text classification\u0017**"
      ],
      "metadata": {
        "id": "fmp80Ny7RkOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "\n",
        "# Sample text data and labels\n",
        "texts = [\"I love this movie\", \"This film was terrible\", \"Amazing performance\", \"I did not like the plot\"]\n",
        "labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Build GRU model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=16, input_length=5),\n",
        "    GRU(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, labels, epochs=10, verbose=1)\n",
        "\n",
        "# Test on a new sentence\n",
        "test_text = [\"I really enjoyed the movie\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=5)\n",
        "prediction = model.predict(test_pad)\n",
        "print(\"Predicted Probability of Positive Sentiment:\", prediction[0][0])\n"
      ],
      "metadata": {
        "id": "rrt6iBawRod3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. How do you implement a text generation model using LSTM with Keras\u0017**"
      ],
      "metadata": {
        "id": "413g4xxER-q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\"hello world\", \"hello there\", \"hello Keras\", \"world of AI\", \"Keras is fun\"]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences (n-grams)\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_seq = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_seq)\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# Split predictors and label\n",
        "X = input_sequences[:,:-1]\n",
        "y = input_sequences[:,-1]\n",
        "\n",
        "# One-hot encode the labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=10, input_length=max_seq_len-1),\n",
        "    LSTM(50),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"hello\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ],
      "metadata": {
        "id": "Fb1uddW0RtMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.How do you implement a simple Bi-directional GRU for sequence labeling?**"
      ],
      "metadata": {
        "id": "hVnHBZP6SQW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, TimeDistributed, Dense\n",
        "\n",
        "# Sample tokenized sequences (e.g., words represented as integers)\n",
        "X = [[1, 2, 3], [4, 5, 6, 7], [8, 9]]\n",
        "# Corresponding labels for sequence labeling (e.g., POS tags)\n",
        "y = [[0, 1, 0], [1, 0, 1, 0], [0, 1]]\n",
        "\n",
        "# Pad sequences\n",
        "max_len = max(len(seq) for seq in X)\n",
        "X_pad = pad_sequences(X, maxlen=max_len, padding='post')\n",
        "y_pad = pad_sequences(y, maxlen=max_len, padding='post')\n",
        "\n",
        "# One-hot encode labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "num_classes = 2  # number of label classes\n",
        "y_pad = np.array([to_categorical(seq, num_classes=num_classes) for seq in y_pad])\n",
        "\n",
        "# Build Bi-directional GRU model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10, output_dim=8, input_length=max_len),  # input_dim = vocab size\n",
        "    Bidirectional(GRU(16, return_sequences=True)),\n",
        "    TimeDistributed(Dense(num_classes, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_pad, y_pad, epochs=20, verbose=1)\n",
        "\n",
        "# Predict on a new sequence\n",
        "test_seq = [[1, 2, 3]]\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "pred = model.predict(test_pad)\n",
        "pred_labels = np.argmax(pred, axis=-1)\n",
        "print(\"Predicted labels:\", pred_labels[0])\n"
      ],
      "metadata": {
        "id": "LFwbhTpzSFOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}