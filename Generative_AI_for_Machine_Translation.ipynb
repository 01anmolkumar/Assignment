{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCBktzokLTdu90EKi4oQtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01anmolkumar/Assignment/blob/main/Generative_AI_for_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Statistical Machine Translation (SMT)?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Statistical Machine Translation (SMT) is an approach to machine translation that uses statistical\n",
        "  models to translate text from one language to another.\n",
        "* It is based on analyzing large bilingual corpora to learn translation patterns between source\n",
        "  and target languages.\n",
        "* SMT calculates the probability of a target sentence given a source sentence.\n",
        "* It uses components such as translation models, language models, and alignment models.\n",
        "* SMT typically performs word-based or phrase-based translation using probability distributions.\n",
        "* This approach was widely used before the rise of Neural Machine Translation (NMT).\n"
      ],
      "metadata": {
        "id": "9hlcvGmAg7XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the main differences between SMT and Neural Machine Translation (NMT)?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Statistical Machine Translation (SMT) uses probabilistic and rule-based components such as\n",
        "  language models, translation models, and word alignments.\n",
        "* Neural Machine Translation (NMT) uses deep learning and neural networks to perform\n",
        "  end-to-end translation.\n",
        "* SMT translates text word-by-word or phrase-by-phrase, while NMT translates entire\n",
        "  sentences as a single sequence.\n",
        "* NMT captures context and semantic meaning more effectively than SMT.\n",
        "* SMT often produces fragmented or less fluent translations, whereas NMT generates\n",
        "  more natural and human-like translations.\n",
        "* NMT performs better on long sentences compared to SMT.\n"
      ],
      "metadata": {
        "id": "MF8sk3ymhaAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Explain the concept of attention in Neural Machine Translation**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Attention is a mechanism in Neural Machine Translation that allows the model to focus on\n",
        "  specific parts of the source sentence while generating each word in the target sentence.\n",
        "* Instead of encoding the entire source sentence into a single fixed-length vector, attention\n",
        "  dynamically assigns importance weights to different source words.\n",
        "* This helps the model capture relevant context during translation.\n",
        "* Attention improves translation accuracy, especially for long and complex sentences.\n",
        "* It enables better alignment between source and target words.\n"
      ],
      "metadata": {
        "id": "XrQCFKG-heaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. How do Generative Pre-trained Transformers (GPTs) contribute to machine translation?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Generative Pre-trained Transformers (GPTs) are large language models trained on vast amounts\n",
        "  of multilingual and monolingual text data.\n",
        "* GPTs learn grammar, semantics, and contextual relationships between words during pre-training.\n",
        "* They can perform machine translation through fine-tuning or prompt-based learning.\n",
        "* GPTs generate fluent and context-aware translations without explicit rule-based systems.\n",
        "* Their transformer architecture allows them to handle long-range dependencies effectively.\n"
      ],
      "metadata": {
        "id": "x5uaR04QhkaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is poetry generation in generative AI?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Poetry generation in generative AI refers to the use of AI models to automatically create poems.\n",
        "* These models learn poetic structures, rhyme schemes, rhythm, and emotional expression from\n",
        "  large datasets of existing poetry.\n",
        "* Generative models predict word sequences that follow poetic patterns.\n",
        "* Transformer-based models like GPT are commonly used for poetry generation.\n",
        "* This technique is applied in creative writing, entertainment, and artistic exploration.\n"
      ],
      "metadata": {
        "id": "Lx_0GY2rhnxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. How does music composition with generative AI work?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Music composition with generative AI involves training models on musical data such as notes,\n",
        "  chords, melodies, and rhythms.\n",
        "* The models learn patterns and structures present in existing music.\n",
        "* Generative AI predicts sequences of musical elements to create new compositions.\n",
        "* Techniques such as recurrent neural networks, transformers, and GANs are commonly used.\n",
        "* This approach is used for background music generation, creative composition, and sound design.\n"
      ],
      "metadata": {
        "id": "gz4XDPazhpzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. What role does reinforcement learning play in generative AI for NLP?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Reinforcement learning helps improve generative AI models by optimizing their outputs based\n",
        "  on feedback or reward signals.\n",
        "* The model generates text and receives rewards for producing desirable responses.\n",
        "* It allows models to learn through trial and error rather than only labeled data.\n",
        "* Reinforcement learning is often used to fine-tune language models for better quality,\n",
        "  coherence, and relevance.\n",
        "* It is commonly applied in dialogue systems and text generation tasks.\n"
      ],
      "metadata": {
        "id": "DPrUtFeWhu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What are multimodal generative models?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Multimodal generative models are AI models that can process and generate data from\n",
        "  multiple types of modalities.\n",
        "* These modalities include text, images, audio, and video.\n",
        "* The models learn relationships and interactions between different data types.\n",
        "* They can generate one modality from another, such as generating captions from images.\n",
        "* Multimodal models are widely used in applications like image captioning, visual question\n",
        "  answering, and creative content generation.\n"
      ],
      "metadata": {
        "id": "iNuT-oI2hz2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Define Natural Language Understanding (NLU) in the context of generative AI**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Natural Language Understanding (NLU) is the ability of generative AI systems to comprehend\n",
        "  the meaning and intent of human language.\n",
        "* It involves understanding context, semantics, syntax, and relationships between words.\n",
        "* NLU enables models to interpret user input accurately before generating responses.\n",
        "* In generative AI, NLU helps produce relevant, coherent, and meaningful content.\n",
        "* It is essential for applications such as chatbots, virtual assistants, and machine translation.\n"
      ],
      "metadata": {
        "id": "oLtcz4rRh5Vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What ethical considerations arise in generative AI for creative writing?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Ethical concerns include plagiarism and the risk of copying existing creative works.\n",
        "* There is a possibility of generating biased or harmful content.\n",
        "* Authorship and ownership of AI-generated content raise legal and moral questions.\n",
        "* Generative AI may spread misinformation if not properly controlled.\n",
        "* Responsible use and proper guidelines are required to ensure ethical creative writing.\n"
      ],
      "metadata": {
        "id": "WuRaKxnfh-vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. How can attention mechanisms improve NMT performance on longer sentences?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Attention mechanisms allow NMT models to focus on relevant parts of the source sentence\n",
        "  while translating each target word.\n",
        "* They prevent information loss that occurs when encoding long sentences into a single vector.\n",
        "* Attention helps maintain alignment between source and target words.\n",
        "* It improves handling of long-range dependencies in longer sentences.\n",
        "* As a result, translations become more accurate and context-aware.\n"
      ],
      "metadata": {
        "id": "GcX3lKpfiFKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13. Explain how reinforcement learning differs from supervised learning in generative AI**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* Reinforcement learning trains models using reward signals instead of labeled input-output pairs.\n",
        "* The model learns through interaction and trial-and-error.\n",
        "* Supervised learning relies on large labeled datasets for training.\n",
        "* Reinforcement learning focuses on optimizing long-term rewards.\n",
        "* In generative AI, reinforcement learning improves output quality and user alignment.\n"
      ],
      "metadata": {
        "id": "1yqXPjWUiJik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14. What is the role of a decoder in NMT models?**\n",
        "\n",
        "Answer :-\n",
        "\n",
        "* The decoder is a component of Neural Machine Translation models responsible for generating\n",
        "  the target language sentence.\n",
        "* It produces one word at a time based on previous generated words and encoded source information.\n",
        "* The decoder uses attention mechanisms to focus on relevant parts of the source sentence.\n",
        "* It ensures grammatical structure and fluency in the translated output.\n",
        "* The decoder plays a key role in producing accurate and coherent translations.\n"
      ],
      "metadata": {
        "id": "Ycl7cnsoiSZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.How does fine-tuning a GPT model differ from pre-training it**\n",
        "\n",
        "* Pre-training is done on very large, diverse datasets to learn general language patterns and knowledge.  \n",
        "* Fine-tuning is performed on smaller, task-specific or domain-specific datasets.  \n",
        "* Pre-training teaches the model grammar, facts, and general language understanding.  \n",
        "* Fine-tuning adapts this general knowledge to a specific task or use case.  \n",
        "* Pre-training requires massive computational resources and time.  \n",
        "* Fine-tuning is faster and less resource-intensive.  \n",
        "* During pre-training, the model learns from scratch or near-scratch.  \n",
        "* During fine-tuning, the model updates existing weights rather than learning everything again.  \n",
        "* Pre-training enables broad capabilities, while fine-tuning improves accuracy and relevance for specific applications.  \n"
      ],
      "metadata": {
        "id": "Tn4gZpbuio1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.Describe one approach generative AI uses to avoid overfitting in creative content generation**\n",
        "\n",
        "* One common approach is the use of regularization techniques during training.  \n",
        "* Methods like dropout randomly disable neurons, preventing reliance on specific patterns.  \n",
        "* This encourages the model to learn more general and robust representations.  \n",
        "* Regularization reduces memorization of training examples.  \n",
        "* It helps the model generate diverse and novel creative content.  \n",
        "* Prevents repetitive or overly similar outputs.  \n",
        "* Improves generalization to unseen prompts or ideas.  \n",
        "* Supports balanced creativity and consistency in generated content.  \n"
      ],
      "metadata": {
        "id": "pRQ_YMtGizMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.What makes GPT-based models effective for creative storytelling**\n",
        "\n",
        "* GPT models are trained on large and diverse text datasets, exposing them to many storytelling styles and structures.  \n",
        "* Self-attention allows the model to maintain context across long narratives.  \n",
        "* They can generate coherent sequences by predicting the next token based on prior context.  \n",
        "* Probabilistic text generation enables creativity and variation in stories.  \n",
        "* The models understand characters, plots, and narrative flow from learned patterns.  \n",
        "* Prompt engineering allows control over tone, genre, and style.  \n",
        "* Large context windows help maintain consistency throughout a story.  \n",
        "* Fine-tuning can further enhance storytelling for specific genres or audiences.  \n"
      ],
      "metadata": {
        "id": "D2zCOXxbjGWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.How does context preservation work in Neural Machine Translation (NMT) models**\n",
        "\n",
        "* NMT models use encoder–decoder architectures to capture the meaning of the entire source sentence.  \n",
        "* The encoder converts the input sentence into contextual representations rather than word-by-word translations.  \n",
        "* Attention mechanisms allow the decoder to focus on relevant parts of the source sentence while generating each target word.  \n",
        "* Self-attention in transformer-based NMT models captures relationships between all words in a sentence.  \n",
        "* Context from long sentences is preserved by considering global dependencies, not just nearby words.  \n",
        "* Bidirectional encoding helps understand both past and future context in the source language.  \n",
        "* This approach reduces ambiguity and improves translation accuracy.  \n",
        "* Overall, context preservation ensures fluent, coherent, and semantically correct translations.  \n"
      ],
      "metadata": {
        "id": "-b1u73ukjRNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.What is the main advantage of multimodal models in creative applications**\n",
        "\n",
        "* Multimodal models can process and combine multiple data types such as text, images, audio, and video.  \n",
        "* They enable richer and more expressive creative outputs by integrating different modalities.  \n",
        "* Text prompts can be transformed into visual, audio, or mixed-media content.  \n",
        "* Cross-modal understanding improves coherence between different creative elements.  \n",
        "* They allow more intuitive and natural human–AI interaction.  \n",
        "* Multimodal models support innovative applications like text-to-image art and video generation.  \n",
        "* Combining modalities enhances storytelling and artistic expression.  \n",
        "* This integration leads to more immersive and engaging creative experiences.  \n"
      ],
      "metadata": {
        "id": "JZPEL9TIjcRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.How does generative AI handle cultural nuances in translation**\n",
        "\n",
        "* Generative AI learns cultural patterns from large, diverse multilingual datasets.  \n",
        "* It captures idioms, phrases, and context-specific expressions used in different cultures.  \n",
        "* Context-aware models adjust translations based on sentence meaning rather than literal word mapping.  \n",
        "* Attention mechanisms help preserve tone, formality, and intent across languages.  \n",
        "* Fine-tuning on region-specific data improves cultural sensitivity.  \n",
        "* Prompt instructions can guide the model to adapt translations for specific audiences.  \n",
        "* Multilingual pre-training helps recognize cultural references and norms.  \n",
        "* Human feedback is often used to correct and refine culturally appropriate translations.  \n"
      ],
      "metadata": {
        "id": "5cVrE9Uujoqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21.Why is it difficult to fully remove bias in generative AI models**\n",
        "\n",
        "* Generative AI models learn from large datasets that often reflect real-world biases.  \n",
        "* Biases can be subtle, implicit, and deeply embedded in language and data.  \n",
        "* Completely filtering biased data may remove important context or knowledge.  \n",
        "* Cultural and societal norms vary, making bias definition subjective.  \n",
        "* Models generalize patterns, which can unintentionally amplify biases.  \n",
        "* Human annotations and feedback can also introduce bias.  \n",
        "* Trade-offs exist between fairness, accuracy, and usefulness.  \n",
        "* Continuous monitoring and mitigation are required, but total removal is challenging.  \n"
      ],
      "metadata": {
        "id": "PEMQDj6hj3Sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "Ja-xZrommHWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.Implement a basic Statistical Machine Translation (SMT) model that uses word-by-word translation with a\n",
        "dictionary lookup approach**"
      ],
      "metadata": {
        "id": "XUhbEzoimByU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VH5E4pyg2F4"
      },
      "outputs": [],
      "source": [
        "# English to French dictionary\n",
        "dictionary = {\n",
        "    \"hello\": \"bonjour\",\n",
        "    \"how\": \"comment\",\n",
        "    \"are\": \"êtes\",\n",
        "    \"you\": \"vous\",\n",
        "    \"i\": \"je\",\n",
        "    \"love\": \"aime\",\n",
        "    \"nature\": \"nature\"\n",
        "}\n",
        "\n",
        "def smt_translate(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    translated_words = [dictionary.get(word, word) for word in words]\n",
        "    return \" \".join(translated_words)\n",
        "\n",
        "# Example\n",
        "print(smt_translate(\"hello how are you\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.Implement an Attention mechanism in a Neural Machine Translation (NMT) model using PyTorch**"
      ],
      "metadata": {
        "id": "ASk88fHomqKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (1, batch_size, hidden_size)\n",
        "        # encoder_outputs: (seq_len, batch_size, hidden_size)\n",
        "\n",
        "        seq_len = encoder_outputs.size(0)\n",
        "        hidden = hidden.repeat(seq_len, 1, 1)\n",
        "\n",
        "        energy = torch.tanh(\n",
        "            self.attn(torch.cat((hidden, encoder_outputs), dim=2))\n",
        "        )\n",
        "\n",
        "        attention_weights = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention_weights, dim=0)\n"
      ],
      "metadata": {
        "id": "_F4sDlarmXix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.Use a pre-trained GPT model to perform machine translation from English to French**"
      ],
      "metadata": {
        "id": "B7EI4X0gm7uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained English to French translation model\n",
        "translator = pipeline(\n",
        "    \"translation_en_to_fr\",\n",
        "    model=\"Helsinki-NLP/opus-mt-en-fr\"\n",
        ")\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"Artificial intelligence is very powerful.\"\n",
        "\n",
        "# Perform translation\n",
        "translation = translator(sentence)\n",
        "\n",
        "print(translation[0][\"translation_text\"])\n"
      ],
      "metadata": {
        "id": "Wjh65o1pmwdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4Generate a short poem using GPT-2 for a specific theme (e.g., \"Nature\")**"
      ],
      "metadata": {
        "id": "fioF-J3QnrP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Prompt for poem generation\n",
        "prompt = \"Write a short poem about nature:\\n\"\n",
        "\n",
        "# Encode input\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(\n",
        "    inputs,\n",
        "    max_length=60,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Decode and print poem\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "W848FrSTnejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Q5.Implement a basic reinforcement learning setup for text generation using PyTorch's reward function**"
      ],
      "metadata": {
        "id": "O_wvakHb_lF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Reward function\n",
        "def reward_function(text):\n",
        "    return 1.0 if \"nature\" in text.lower() else 0.0\n",
        "\n",
        "# Example generated text (action)\n",
        "generated_text = \"Nature makes life peaceful\"\n",
        "\n",
        "# Get reward\n",
        "reward = reward_function(generated_text)\n",
        "\n",
        "# Convert reward to loss (policy gradient idea)\n",
        "loss = -torch.tensor(reward, requires_grad=True)\n",
        "\n",
        "# Backpropagation step\n",
        "loss.backward()\n",
        "\n",
        "print(\"Generated Text:\", generated_text)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "id": "vG9uwG5uoDTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.Create a simple multimodal generative model that generates an image caption given an image**"
      ],
      "metadata": {
        "id": "ETv5daZjAlWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# 1️⃣ Feature extractor: Pre-trained CNN (ResNet)\n",
        "cnn = models.resnet18(pretrained=True)\n",
        "cnn.fc = nn.Identity()  # Remove final classification layer\n",
        "\n",
        "# 2️⃣ Caption generator: LSTM\n",
        "class CaptionGenerator(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions=None):\n",
        "        # features: (batch, embed_size)\n",
        "        features = features.unsqueeze(1)  # (batch, 1, embed_size)\n",
        "        outputs, _ = self.lstm(features)  # LSTM step\n",
        "        outputs = self.fc(outputs)        # Generate vocab scores\n",
        "        return outputs\n",
        "\n",
        "# 3️⃣ Example usage\n",
        "dummy_image = torch.randn(1, 3, 224, 224)  # Random image\n",
        "features = cnn(dummy_image)                 # Extract features\n",
        "\n",
        "vocab_size = 1000\n",
        "model = CaptionGenerator(embed_size=features.size(1), hidden_size=256, vocab_size=vocab_size)\n",
        "output = model(features)\n",
        "\n",
        "print(\"Output shape (batch, seq_len, vocab_size):\", output.shape)\n"
      ],
      "metadata": {
        "id": "0gY_0efj_79c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.Demonstrate how to evaluate bias in generated content by analyzing GPT responses to prompts with\n",
        "potentially sensitive terms**"
      ],
      "metadata": {
        "id": "WwRuAiy0AxVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 text generation model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Prompts with potentially sensitive terms\n",
        "prompts = [\n",
        "    \"Describe a doctor:\",\n",
        "    \"Describe a nurse:\",\n",
        "    \"Describe a programmer:\"\n",
        "]\n",
        "\n",
        "# Generate and analyze responses\n",
        "for prompt in prompts:\n",
        "    output = generator(prompt, max_length=50, do_sample=True)[0][\"generated_text\"]\n",
        "    print(f\"\\nPrompt: {prompt}\\nGenerated Text: {output}\")\n"
      ],
      "metadata": {
        "id": "ZzGzaFzVAsgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.Create a simple Neural Machine Translation model with PyTorch for translating English phrases to German.**"
      ],
      "metadata": {
        "id": "uCjhzvEvBDvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)         # (batch, seq_len, embed_dim)\n",
        "        outputs, hidden = self.lstm(embedded)\n",
        "        return outputs, hidden               # outputs for attention, hidden for decoder\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)         # (batch, 1, embed_dim)\n",
        "        outputs, hidden = self.lstm(embedded, hidden)\n",
        "        predictions = self.fc(outputs)       # (batch, seq_len, vocab_size)\n",
        "        return predictions, hidden\n",
        "\n",
        "# Example usage\n",
        "input_vocab_size = 5000\n",
        "output_vocab_size = 5000\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embed_dim, hidden_dim)\n",
        "decoder = Decoder(output_vocab_size, embed_dim, hidden_dim)\n",
        "\n",
        "# Dummy input (batch_size=1, seq_len=5)\n",
        "english_input = torch.randint(0, input_vocab_size, (1,5))\n",
        "encoder_outputs, encoder_hidden = encoder(english_input)\n",
        "\n",
        "# Decoder input (start token)\n",
        "german_input = torch.randint(0, output_vocab_size, (1,1))\n",
        "decoder_outputs, decoder_hidden = decoder(german_input, encoder_hidden)\n",
        "\n",
        "print(\"Decoder output shape:\", decoder_outputs.shape)  # (batch, seq_len, vocab_size)\n"
      ],
      "metadata": {
        "id": "KRxPqGpFA-gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fX8BlsbnBTxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}